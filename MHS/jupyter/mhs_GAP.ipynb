{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%writefile gap.py\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import os as os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mhsdir = Path(os.getcwd()).parent\n",
    "sys.path.append(os.path.join(mhsdir, 'src'))\n",
    "\n",
    "import ns\n",
    "import gap\n",
    "\n",
    "datarawdir = os.path.join(mhsdir, 'data\\\\raw\\\\YC')\n",
    "dataprepdir = os.path.join(mhsdir, 'data\\\\preprocessed')\n",
    "dataprepnsdir = os.path.join(mhsdir, 'data\\\\preprocessed\\\\ns')\n",
    "\n",
    "tempdatarawdir = os.path.join(mhsdir, 'temp\\\\data\\\\raw\\\\YC')\n",
    "tempdataprepdir = os.path.join(mhsdir, 'temp\\\\data\\\\preprocessed')\n",
    "tempdataprepnsdir = os.path.join(mhsdir, 'temp\\\\data\\\\preprocessed\\\\ns')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restriction = tfk.constraints.NonNeg()\n",
    "BATCH_SIZE = 32\n",
    "dataset = gap.make_dataset(dataprepnsdir, eps = 0.00001).batch(BATCH_SIZE)\n",
    "model = gap.GAPNSModel(event_shape = 5, gwide = 32, gdeep = 5, dwide = 32, ddeep = 5, penalty = 0.01, kernel_reg = gap.custom_reg)\n",
    "model.fit(dataset, epochs = 1, batch = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0,5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 24.0, 36.0, 48.0, 60.0, 72.0, \n",
    "              84.0, 96.0, 108.0, 120.0, 144.0, 180.0, 240.0, 300.0, 360.0, 480.0, 600.0])  \n",
    "x, y, _ = gap.generate_ycs(model)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gap.py\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "cross_entropy = tfk.losses.BinaryCrossentropy(from_logits=True)\n",
    "BATCH_SIZE = 16\n",
    "noise_dim = 5\n",
    "gauss_params = [0.0, 1.0]\n",
    "generator_inputs = np.array([gauss_params] * BATCH_SIZE)\n",
    "\n",
    "class StrictlyPositive(tfk.constraints.Constraint):\n",
    "    def __init__(self, eps = 0.00001):\n",
    "        self.eps = tf.constant(eps)\n",
    "    def __call__(self, w):\n",
    "        con = tfk.constraints.NonNeg()\n",
    "        return self.eps + con(w)\n",
    "    \n",
    "restriction = tfk.constraints.NonNeg()\n",
    "\n",
    "\n",
    "def make_dataset(srcdir, eps = 0.00001):\n",
    "    dataset = None\n",
    "    for filename in os.listdir(srcdir):\n",
    "        df = pd.read_csv(os.path.join(srcdir,filename), usecols = ['Thau','Alpha0','Alpha1','Alpha2'])\n",
    "        lx = df['Thau'].notnull()\n",
    "        m = df[lx][['Thau','Alpha0','Alpha1','Alpha2']].to_numpy() + eps\n",
    "        if dataset is None:\n",
    "            dataset = m\n",
    "        else:\n",
    "            dataset = np.concatenate([dataset,m], axis = 0)\n",
    "    return tf.data.Dataset.from_tensor_slices(dataset).shuffle(1000000)\n",
    "\n",
    "\n",
    "\n",
    "generator = make_generator_model(event_shape = noise_dim, alpha = 0.3, wide = 32, deep = 4, weight_restriction = restriction)\n",
    "discriminator = make_discriminator_model(wide = 32, deep = 4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(nsv):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_nsv = generator(generator_inputs, training=True)\n",
    "        \n",
    "        real_output = discriminator(nsv, training=True)\n",
    "        fake_output = discriminator(generated_nsv, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch - {}'.format(epoch))\n",
    "        for ns_batch in dataset:\n",
    "            train_step(ns_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.25, 0.5, 1.0, 2.0, 3.0, 4.0,5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 24.0, 36.0, 48.0, 60.0, 72.0, \n",
    "              84.0, 96.0, 108.0, 120.0, 144.0, 180.0, 240.0, 300.0, 360.0, 480.0, 600.0])  \n",
    "seed = np.array([gauss_params])\n",
    "nsl = ns.NelsonSiegelLayer()\n",
    "nsp = None\n",
    "for _ in range(1000):\n",
    "    nsp = generator(seed,training = False)\n",
    "    nsp = tf.squeeze(nsp)\n",
    "    if nsp[0] > 10.0 and nsp[0] < 120.0:\n",
    "        break\n",
    "nsl.assignValues(nsp)\n",
    "y = nsl(x)\n",
    "plt.plot(x,y)\n",
    "print(nsl.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = tfk.models.load_model(\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
