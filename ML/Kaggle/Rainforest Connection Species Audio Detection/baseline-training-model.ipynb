{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp\n",
    "import sklearn as sl\n",
    "import pandas as pd\n",
    "#import librosa\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import tensorflow.data as tfd\n",
    "#import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import os\n",
    "'''\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "'''\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(os.getcwd(),'submission_mix')\n",
    "[os.path.join(root,file) for file in os.listdir(root) if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e24_s2048_ms05_f16000_u256.csv\n",
      "1 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e32_s2048_ms05_f16000_u256.csv\n",
      "2 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e36_s2048_ms05_f16000_u256.csv\n",
      "3 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e40_s2048_ms05_f16000_u256.csv\n",
      "4 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e40_s2048_ms05_f18000_u256.csv\n",
      "5 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e44_s2048_ms05_f16000_u256.csv\n",
      "6 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e44_s2048_ms05_f18000_u256.csv\n",
      "7 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e48_s2048_ms05_f16000_u256.csv\n",
      "8 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e48_s2048_ms05_f18000_u256.csv\n",
      "9 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e52_s2048_ms05_f16000_u256.csv\n",
      "10 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e52_s2048_ms05_f18000_u256.csv\n",
      "11 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e56_s2048_ms05_f16000_u256.csv\n",
      "12 -- C:\\Users\\PLDD\\Practice\\GitHub\\Python\\ML\\Kaggle\\Rainforest Connection Species Audio Detection\\s1\\submission_v6_e56_s2048_ms05_f18000_u256.csv\n"
     ]
    }
   ],
   "source": [
    "root = os.path.join(os.getcwd(),'submission_mix')\n",
    "e = '64_68_72'\n",
    "version = 7\n",
    "files = [os.path.join(root,f'e{e}_f16000.csv'), os.path.join(root,f'e{e}_f18000.csv'),\n",
    "         os.path.join(root,f'e{e}_f20000.csv'), os.path.join(root,f'e{e}_f22000.csv')]\n",
    "files = [os.path.join(root,f) for f in os.listdir(root) if os.path.isfile(os.path.join(root, f))]\n",
    "cols = ['s{}'.format(i) for i in range(24)]\n",
    "df = pd.read_csv(files[0])\n",
    "mm = None\n",
    "for i,file in enumerate(files):\n",
    "    print(f\"{i} -- {file}\")\n",
    "    dff = pd.read_csv(file)\n",
    "    m = dff[cols].to_numpy()\n",
    "    if i == 0:\n",
    "        mm = m\n",
    "    else:\n",
    "        mm = np.maximum(mm,m)\n",
    "df[cols] = mm\n",
    "df.to_csv(f'submission_mix_v{version}_e{e}_fpf02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join(os.getcwd(),'submission_mix_v6_threshold')\n",
    "th = 0.5\n",
    "version = 6\n",
    "filename = os.path.join(root,'submission_v6_mix_ms05_u256_f16_f18_e40_e64.csv')\n",
    "cols = ['s{}'.format(i) for i in range(24)]\n",
    "df = pd.read_csv(filename)\n",
    "m = df[cols].to_numpy()\n",
    "m = m > th\n",
    "df[cols] = np.float32(m)\n",
    "df.to_csv(f's1_05.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>s0</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>...</th>\n",
       "      <th>s14</th>\n",
       "      <th>s15</th>\n",
       "      <th>s16</th>\n",
       "      <th>s17</th>\n",
       "      <th>s18</th>\n",
       "      <th>s19</th>\n",
       "      <th>s20</th>\n",
       "      <th>s21</th>\n",
       "      <th>s22</th>\n",
       "      <th>s23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000316da7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003bc2cb2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0061c037e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010eb14d3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>011318064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>ff68f3ac3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>ff973e852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>ffa5cf6d6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>ffa88cbb8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>ffda5d7b3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1992 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     recording_id   s0   s1   s2   s3   s4   s5   s6   s7   s8  ...  s14  s15  \\\n",
       "0       000316da7  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1       003bc2cb2  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0   \n",
       "2       0061c037e  0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  ...  0.0  0.0   \n",
       "3       010eb14d3  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  0.0  0.0   \n",
       "4       011318064  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  1.0  1.0   \n",
       "...           ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1987    ff68f3ac3  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0   \n",
       "1988    ff973e852  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  1.0   \n",
       "1989    ffa5cf6d6  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  1.0   \n",
       "1990    ffa88cbb8  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0   \n",
       "1991    ffda5d7b3  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "      s16  s17  s18  s19  s20  s21  s22  s23  \n",
       "0     0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "1     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "3     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "1987  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  \n",
       "1988  0.0  0.0  0.0  0.0  1.0  0.0  1.0  1.0  \n",
       "1989  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "1990  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1991  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
       "\n",
       "[1992 rows x 25 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv('times1.csv',names = ['nodb', 'db'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'parse_params': {\n",
    "        'sample_rate' : 48000,\n",
    "        'cut' : 8\n",
    "    },\n",
    "    'data_params': {\n",
    "        'frame_length' : 2048,\n",
    "        'frame_step' : 128,\n",
    "        'mel_highest_freq' : 24000,\n",
    "        'mel_lowest_freq' : 32,\n",
    "        'mel_num_bins' : 512,\n",
    "        'img_shape' : (512, 256)\n",
    "    },\n",
    "    'model_params': {\n",
    "        'loss': {\n",
    "            'fn': tfa.losses.SigmoidFocalCrossEntropy,\n",
    "            'params': {},\n",
    "        },\n",
    "        'optim': {\n",
    "            'fn': tfa.optimizers.RectifiedAdam,\n",
    "            'params': {'lr': 1e-3, 'total_steps': 15*64, 'warmup_proportion': 0.3, 'min_lr': 1e-6},\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_N = 24\n",
    "\n",
    "SR = cfg['parse_params']['sample_rate']     # all wave's sample rate may be 48k\n",
    "CUT = cfg['parse_params']['cut']\n",
    "INPUT_SEQ_SIZE = int(SR * CUT)\n",
    "\n",
    "FRM_LEN = cfg['data_params']['frame_length']\n",
    "FRM_STEP = cfg['data_params']['frame_step']\n",
    "MEL_FRQ_H = cfg['data_params']['mel_highest_freq']\n",
    "MEL_FRQ_L = cfg['data_params']['mel_lowest_freq']\n",
    "MEL_BINS = cfg['data_params']['img_shape'][0]\n",
    "IMG_SIZE_H = cfg['data_params']['img_shape'][0]\n",
    "IMG_SIZE_W = cfg['data_params']['img_shape'][1]\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "TRAIN_TFREC = GCS_DS_PATH + \"/tfrecords/train\"\n",
    "TEST_TFREC = GCS_DS_PATH + \"/tfrecords/test\"\n",
    "ALL_TRAIN_TFREC = [os.path.join(TRAIN_TFREC,filename) for filename in tf.io.gfile.listdir(TRAIN_TFREC)]\n",
    "ALL_TEST_TFREC = [os.path.join(TEST_TFREC,filename) for filename in tf.io.gfile.listdir(TEST_TFREC)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define YAMNET model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)  # Instances of this class are immutable.\n",
    "class Params:\n",
    "    sample_rate: float = SR\n",
    "    frm_len: int = FRM_LEN\n",
    "    frm_step: int = FRM_STEP\n",
    "    mel_bands: int = MEL_BINS\n",
    "    mel_min_hz: float = MEL_FRQ_L\n",
    "    mel_max_hz: float = MEL_FRQ_H\n",
    "    log_offset: float = 0.001\n",
    "    img_size_h: int = IMG_SIZE_H\n",
    "    img_size_w: int = IMG_SIZE_W\n",
    "    input_signal_len: int = INPUT_SEQ_SIZE\n",
    "    num_classes: int = CLASS_N\n",
    "    conv_padding: str = 'same'\n",
    "    batchnorm_center: bool = True\n",
    "    batchnorm_scale: bool = False\n",
    "    batchnorm_epsilon: float = 1e-4\n",
    "    classifier_activation: str = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_norm(name, params):\n",
    "  def _bn_layer(layer_input):\n",
    "    return tfkl.BatchNormalization(\n",
    "      name=name,\n",
    "      center=params.batchnorm_center,\n",
    "      scale=params.batchnorm_scale,\n",
    "      epsilon=params.batchnorm_epsilon)(layer_input)\n",
    "  return _bn_layer\n",
    "\n",
    "\n",
    "def _conv(name, kernel, stride, filters, params):\n",
    "  def _conv_layer(layer_input):\n",
    "    output = tfkl.Conv2D(name='{}/conv'.format(name),filters=filters,kernel_size=kernel,strides=stride,\n",
    "                         padding=params.conv_padding,use_bias=False,activation=None)(layer_input)\n",
    "    output = _batch_norm('{}/conv/bn'.format(name), params)(output)\n",
    "    output = tfkl.ReLU(name='{}/relu'.format(name))(output)\n",
    "    return output\n",
    "  return _conv_layer\n",
    "\n",
    "\n",
    "def _separable_conv(name, kernel, stride, filters, params):\n",
    "  def _separable_conv_layer(layer_input):\n",
    "    output = tfkl.DepthwiseConv2D(name='{}/depthwise_conv'.format(name),kernel_size=kernel,strides=stride,depth_multiplier=1,\n",
    "                                  padding=params.conv_padding,use_bias=False,activation=None)(layer_input)\n",
    "    output = _batch_norm('{}/depthwise_conv/bn'.format(name), params)(output)\n",
    "    output = tfkl.ReLU(name='{}/depthwise_conv/relu'.format(name))(output)\n",
    "    output = tfkl.Conv2D(name='{}/pointwise_conv'.format(name),filters=filters,kernel_size=(1, 1),\n",
    "                         strides=1,padding=params.conv_padding,use_bias=False,activation=None)(output)\n",
    "    output = _batch_norm('{}/pointwise_conv/bn'.format(name), params)(output)\n",
    "    output = tfkl.ReLU(name='{}/pointwise_conv/relu'.format(name))(output)\n",
    "    return output\n",
    "  return _separable_conv_layer\n",
    "\n",
    "\n",
    "_YAMNET_LAYER_DEFS = [\n",
    "    # (layer_function, kernel, stride, num_filters)\n",
    "    (_conv,          [3, 3], 2,   32),\n",
    "    (_separable_conv, [3, 3], 1,   64),\n",
    "    (_separable_conv, [3, 3], 2,  128),\n",
    "    (_separable_conv, [3, 3], 1,  128),\n",
    "    (_separable_conv, [3, 3], 2,  256),\n",
    "    (_separable_conv, [3, 3], 1,  256),\n",
    "    (_separable_conv, [3, 3], 2,  512),\n",
    "    (_separable_conv, [3, 3], 1,  512),\n",
    "    (_separable_conv, [3, 3], 1,  512),\n",
    "    (_separable_conv, [3, 3], 1,  512),\n",
    "    (_separable_conv, [3, 3], 1,  512),\n",
    "    (_separable_conv, [3, 3], 1,  512),\n",
    "    (_separable_conv, [3, 3], 2, 1024),\n",
    "    (_separable_conv, [3, 3], 1, 1024)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'recording_id': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'audio_wav': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'label_info': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "}\n",
    "\n",
    "parse_dtype = {\n",
    "    'audio_wav': tf.float32,\n",
    "    'recording_id': tf.string,\n",
    "    'species_id': tf.int32,\n",
    "    'songtype_id': tf.int32,\n",
    "    't_min': tf.float32,\n",
    "    'f_min': tf.float32,\n",
    "    't_max': tf.float32,\n",
    "    'f_max':tf.float32,\n",
    "    'is_tp': tf.int32\n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def parse_step_fun(example_proto):\n",
    "    sample = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    wav, _ = tf.audio.decode_wav(sample['audio_wav'], desired_channels=1) # mono\n",
    "    label_info = tf.strings.split(sample['label_info'], sep='\"')[1]\n",
    "    labels = tf.strings.split(label_info, sep=';')\n",
    "    \n",
    "    @tf.function\n",
    "    def _cut_audio(label):\n",
    "        items = tf.strings.split(label, sep=',')\n",
    "        spid = tf.squeeze(tf.strings.to_number(items[0], tf.int32))\n",
    "        soid = tf.squeeze(tf.strings.to_number(items[1], tf.int32))\n",
    "        tmin = tf.squeeze(tf.strings.to_number(items[2]))\n",
    "        fmin = tf.squeeze(tf.strings.to_number(items[3]))\n",
    "        tmax = tf.squeeze(tf.strings.to_number(items[4]))\n",
    "        fmax = tf.squeeze(tf.strings.to_number(items[5]))\n",
    "        tp = tf.squeeze(tf.strings.to_number(items[6], tf.int32))\n",
    "\n",
    "        tmax_s = tmax * tf.cast(SR, tf.float32)\n",
    "        tmin_s = tmin * tf.cast(SR, tf.float32)\n",
    "        cut_s = tf.cast(CUT * SR, tf.float32)\n",
    "        all_s = tf.cast(60 * SR, tf.float32)\n",
    "        tsize_s = tmax_s - tmin_s\n",
    "        cut_min = tf.cast(\n",
    "            tf.maximum(0.0, \n",
    "                tf.minimum(tmin_s - (cut_s - tsize_s) / 2,\n",
    "                           tf.minimum(tmax_s + (cut_s - tsize_s) / 2, all_s) - cut_s)\n",
    "            ), tf.int32\n",
    "        )\n",
    "        cut_max = cut_min + CUT * SR\n",
    "        \n",
    "        _sample = {\n",
    "            'audio_wav': tf.reshape(wav[cut_min:cut_max], [CUT*SR]),\n",
    "            'recording_id': sample['recording_id'],\n",
    "            'species_id': spid,\n",
    "            'songtype_id': soid,\n",
    "            't_min': tmin - tf.cast(cut_min, tf.float32)/tf.cast(SR, tf.float32),\n",
    "            'f_min': fmin,\n",
    "            't_max': tmax - tf.cast(cut_min, tf.float32)/tf.cast(SR, tf.float32),\n",
    "            'f_max': fmax,\n",
    "            'is_tp': tp\n",
    "        }\n",
    "        return _sample\n",
    "    samples = tf.map_fn(_cut_audio, labels, dtype=parse_dtype)\n",
    "    return samples\n",
    "\n",
    "@tf.function\n",
    "def waveform_to_log_mel_spectrogram(waveform, params):\n",
    "    \"\"\"Compute log mel spectrogram patches of a 1-D waveform.\"\"\"\n",
    "    with tf.name_scope('log_mel_features'):\n",
    "    # Convert waveform into spectrogram using a Short-Time Fourier Transform.\n",
    "    # Note that tf.signal.stft() uses a periodic Hann window by default.\n",
    "        fft_length = 2 ** int(np.ceil(np.log(params.frm_len) / np.log(2.0)))\n",
    "        num_spectrogram_bins = fft_length // 2 + 1\n",
    "        magnitude_spectrogram = tf.abs(tf.signal.stft(signals=waveform,frame_length=params.frm_len,\n",
    "                                                      frame_step=params.frm_step,fft_length=fft_length))\n",
    "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(num_mel_bins=params.mel_bands,\n",
    "                                                                            num_spectrogram_bins=num_spectrogram_bins,\n",
    "                                                                            sample_rate=params.sample_rate,\n",
    "                                                                            lower_edge_hertz=params.mel_min_hz,\n",
    "                                                                            upper_edge_hertz=params.mel_max_hz)\n",
    "        mel_spectrogram = tf.matmul(magnitude_spectrogram, linear_to_mel_weight_matrix)\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + params.log_offset)\n",
    "        return tf.transpose(log_mel_spectrogram,[0,2,1])[...,:params.img_size_w]  \n",
    "    \n",
    "@tf.function #should works as partial function\n",
    "def represent_yamnet_step_fun(sample, params):\n",
    "    seq_size = tf.cast(SR * CUT, tf.int32)\n",
    "    s_min = tf.cast(sample['t_min'] * tf.cast(SR, tf.float32),tf.int32)\n",
    "    s_max = tf.cast(sample['t_max'] * tf.cast(SR, tf.float32),tf.int32)\n",
    "    scut = s_max - s_min\n",
    "    x = tf.pad(tf.slice(sample['audio_wav'],[s_min],[scut]),tf.convert_to_tensor([[0,seq_size - scut]]))\n",
    "    on_val = 1.0\n",
    "    off_val = 0.0\n",
    "    if sample['is_tp'] == 0:\n",
    "        on_val = 0.0\n",
    "        off_val = 1.0 / tf.cast(CLASS_N - 1,tf.float32)\n",
    "    y = tf.one_hot(sample['species_id'],CLASS_N, on_value = on_val, off_value = off_val)\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yamnet_like_model(params):\n",
    "    input = tfk.Input(shape=(params.img_size_h,params.img_size_w), dtype=tf.float32)\n",
    "    x = tfkl.Reshape((params.img_size_h, params.img_size_w,1), input_shape=(params.img_size_h,params.img_size_w))(input)\n",
    "    for (i, (layer_fun, kernel, stride, filters)) in enumerate(_YAMNET_LAYER_DEFS):\n",
    "        x = layer_fun('layer{}'.format(i + 1), kernel, stride, filters, params)(x)\n",
    "    x = tfkl.GlobalAveragePooling2D()(x)\n",
    "    x = tfkl.Dense(units=params.num_classes, use_bias=True)(x)\n",
    "    output = tfkl.Activation(activation=params.classifier_activation)(x)\n",
    "    frames_model = tfk.Model(name='yamnet_frames', inputs=input,outputs=output)\n",
    "    return frames_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _one_sample_positive_class_precisions(example):\n",
    "    y_true, y_pred = example\n",
    "    y_true = tf.cast(y_true > 0, tf.float32)\n",
    "\n",
    "    retrieved_classes = tf.argsort(y_pred, direction='DESCENDING')\n",
    "    class_rankings = tf.argsort(retrieved_classes)\n",
    "    retrieved_class_true = tf.gather(y_true, retrieved_classes)\n",
    "    retrieved_cumulative_hits = tf.math.cumsum(tf.cast(retrieved_class_true, tf.float32))\n",
    "\n",
    "    idx = tf.where(y_true)[:, 0]\n",
    "    i = tf.boolean_mask(class_rankings, y_true)\n",
    "    r = tf.gather(retrieved_cumulative_hits, i)\n",
    "    c = 1 + tf.cast(i, tf.float32)\n",
    "    precisions = r / c\n",
    "\n",
    "    dense = tf.scatter_nd(idx[:, None], precisions, [y_pred.shape[0]])\n",
    "    return dense\n",
    "\n",
    "class LWLRAP(tf.keras.metrics.Metric):\n",
    "    def __init__(self, num_classes, name='lwlrap'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self._precisions = self.add_weight(\n",
    "            name='per_class_cumulative_precision',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "        self._counts = self.add_weight(\n",
    "            name='per_class_cumulative_count',\n",
    "            shape=[num_classes],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        precisions = tf.map_fn(\n",
    "            fn=_one_sample_positive_class_precisions,\n",
    "            elems=(y_true, y_pred),\n",
    "            dtype=(tf.float32),\n",
    "        )\n",
    "\n",
    "        increments = tf.cast(precisions > 0, tf.float32)\n",
    "        total_increments = tf.reduce_sum(increments, axis=0)\n",
    "        total_precisions = tf.reduce_sum(precisions, axis=0)\n",
    "\n",
    "        self._precisions.assign_add(total_precisions)\n",
    "        self._counts.assign_add(total_increments)        \n",
    "\n",
    "    def result(self):\n",
    "        per_class_lwlrap = self._precisions / tf.maximum(self._counts, 1.0)\n",
    "        per_class_weight = self._counts / tf.reduce_sum(self._counts)\n",
    "        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * per_class_weight)\n",
    "        return overall_lwlrap\n",
    "\n",
    "    def reset_states(self):\n",
    "        self._precisions.assign(self._precisions * 0)\n",
    "        self._counts.assign(self._counts * 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = Params()\n",
    "yamnet_fun = partial(represent_yamnet_step_fun, params = par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "model = build_yamnet_like_model(par)\n",
    "optimizer = cfg['model_params']['optim']['fn'](**cfg['model_params']['optim']['params'])\n",
    "model.compile(optimizer=optimizer,loss=tfa.losses.SigmoidFocalCrossEntropy(), metrics=[LWLRAP(CLASS_N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfd.TFRecordDataset(ALL_TRAIN_TFREC).map(parse_step_fun,num_parallel_calls=AUTOTUNE).unbatch()\n",
    "dataset = dataset.map(yamnet_fun, num_parallel_calls=AUTOTUNE).batch(16).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataset)\n",
    "waveform = next(it)\n",
    "\n",
    "sig = waveform[0]\n",
    "model(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfd.TFRecordDataset(ALL_TRAIN_TFREC).map(parse_step_fun,num_parallel_calls=AUTOTUNE).unbatch()\n",
    "dataset = dataset.map(represent_step_fun, num_parallel_calls=AUTOTUNE).batch(16).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = tfd.TFRecordDataset([TRAIN_TFREC + '/00-148.tfrec']).map(parse_step_fun).map(represent_step_fun)\n",
    "s1 = tfd.TFRecordDataset([TRAIN_TFREC + '/01-148.tfrec']).map(parse_step_fun).map(represent_step_fun)\n",
    "s2 = tfd.TFRecordDataset([TRAIN_TFREC + '/02-148.tfrec']).map(parse_step_fun).map(represent_step_fun)\n",
    "s3 = tfd.TFRecordDataset([TRAIN_TFREC + '/03-148.tfrec']).map(parse_step_fun).map(represent_step_fun)\n",
    "ss = [s0, s1, s2, s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../input/rfcx-species-audio-detection/train_tp.csv')\n",
    "df2 = pd.read_csv('../input/rfcx-species-audio-detection/train_fp.csv')\n",
    "print(df1.shape)\n",
    "df = df1.append(df2,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['t_max'] - df['t_min']).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['f_max'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['f_min'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = Params()\n",
    "yamnet_fun = partial(represent_yamnet_step_fun, params = par)\n",
    "\n",
    "dataset = tfd.TFRecordDataset(ALL_TRAIN_TFREC).map(parse_step_fun,num_parallel_calls=AUTOTUNE).unbatch()\n",
    "dataset = dataset.map(yamnet_fun, num_parallel_calls=AUTOTUNE).batch(16).prefetch(AUTOTUNE)\n",
    "\n",
    "it = iter(dataset)\n",
    "waveform = next(it)\n",
    "\n",
    "sig = waveform[0]\n",
    "spect= waveform_to_log_mel_spectrogram(sig, par)\n",
    "\n",
    "#ss = features[0,...]\n",
    "#img = tf.transpose(ss,[1,0]).numpy()\n",
    "img = spect[0,...]\n",
    "plt.imshow(img)\n",
    "\n",
    "features = tf.signal.frame(signal=tf.zeros((4,1024,3,3)),frame_length=512,frame_step=128,axis=0)\n",
    "tf.shape(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(filters = FILTERS, kernel_size = KERNEL_SIZE,\n",
    "                strides = STRIDES, filters_multiplier = 4,\n",
    "                number_of_blocks = NUMBER_OF_BLOCKS):\n",
    "    input = tfk.Input(shape = INPUT_SHAPE)\n",
    "    x = tfkl.Conv1D(filters = filters,  kernel_size = kernel_size, strides=strides, padding = 'same')(input)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    for i in range(number_of_blocks):\n",
    "        x = tfkl.Conv1D(filters = (filters_multiplier*(1 + i)) * filters, kernel_size = 7, padding = 'same')(x)\n",
    "        x = tfkl.LeakyReLU(LEAKY_RELU)(x)\n",
    "        x = tfkl.MaxPooling1D(pool_size=2,strides=2, padding='valid')(x)\n",
    "        x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.GlobalAveragePooling1D()(x)\n",
    "    x = tfkl.Dense(1024, activation='relu')(x)\n",
    "    output = tfkl.Dense(CLASS_N)(x)\n",
    "    model = tfk.Model(inputs = input, outputs = output)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
