{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "VLD_BATCH_SIZE = 50\n",
    "TRAIN_BATCH_SIZE = 30\n",
    "SHUFFLE_BUFFER_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "\n",
    "TRAIN_DIR = './data/pre_512_30/train_images/'\n",
    "TRAIN_DIR = './data/pre/train_images/'\n",
    "\n",
    "TRAIN_DF = pd.read_csv('./data/train.csv')\n",
    "TRAIN_SET = [0,2]\n",
    "TRAIN_CNTS = [900, 900]\n",
    "VLD_CNTS = [99, 99]\n",
    "TRAIN_CNT = 1800\n",
    "VLD_CNT = 198\n",
    "\n",
    "for i in range(100):\n",
    "    TRAIN_DF = TRAIN_DF.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divintosets(df, train, test, classes):\n",
    "    traindic = {'filename' : [], 'class' : []}\n",
    "    testdic = {'filename' : [], 'class' : []}\n",
    "    for i in range(len(classes)):\n",
    "        fn = [x + '.png' for x in df['id_code'][df['diagnosis'] == classes[i]].values]\n",
    "        traindic['filename'] += fn[:train[i]]\n",
    "        traindic['class'] += [str(classes[i])] * train[i]\n",
    "        testdic['filename'] += fn[train[i] : train[i] + test[i]]\n",
    "        testdic['class'] += [str(classes[i])] * test[i]\n",
    "    train_df = pd.DataFrame(data = traindic)\n",
    "    test_df = pd.DataFrame(data = testdic)\n",
    "    for i in range(10):\n",
    "        train_df = train_df.sample(frac = 1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac = 1).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = divintosets(TRAIN_DF, TRAIN_CNTS, VLD_CNTS, TRAIN_SET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 validated image filenames belonging to 2 classes.\n",
      "Found 198 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "tr_gen = gen.flow_from_dataframe(train_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                 batch_size = TRAIN_BATCH_SIZE\n",
    "                                 )\n",
    "vld_gen = gen.flow_from_dataframe(test_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                  batch_size = 1,\n",
    "                                  shuffle = False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 109, 109, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 86528)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               44302848  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 44,396,609\n",
      "Trainable params: 44,396,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 60 steps, validate for 3 steps\n",
      "Epoch 1/3\n",
      "60/60 [==============================] - 127s 2s/step - loss: 0.3941 - accuracy: 0.8322 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "60/60 [==============================] - 108s 2s/step - loss: 0.2484 - accuracy: 0.9000 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "60/60 [==============================] - 123s 2s/step - loss: 0.2329 - accuracy: 0.9200 - val_loss: 0.0634 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(tr_gen,\n",
    "                    steps_per_epoch = TRAIN_CNT//TRAIN_BATCH_SIZE,\n",
    "                    epochs = 3,\n",
    "                    validation_data = vld_gen,\n",
    "                    validation_steps = VLD_CNT//VLD_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "pred_df = TRAIN_DF.copy()\n",
    "v = [x + '.png' for x in pred_df['id_code'].values]\n",
    "pred_df['id_code'] = v\n",
    "v = [str(x%2) for x in pred_df['diagnosis'].values]\n",
    "pred_df['diagnosis'] = v\n",
    "pred_df.columns = ['filename','class']\n",
    "pred_gen = gen.flow_from_dataframe(pred_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                  batch_size = 64,\n",
    "                                  shuffle = False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.63593435287476\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "vv = model.predict(pred_gen)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c1,c2 in vld_gen:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.reshape(224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR + test_df['filename'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = TRAIN_DIR + test_df['filename'][0]\n",
    "img = tf.io.read_file(file_path)\n",
    "img = tf.image.decode_png(img, channels=3)\n",
    "img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "#img = (img/255)\n",
    "#model.predict(tf.reshape(img,(1,224,224,3)))[0,0]\n",
    "#test_df['class'][0]\n",
    "print(\"min = {}, max = {}\".format(np.min(img), np.max(img)))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {'p' : [], 'c' : []}\n",
    "acc = 0.0\n",
    "for i in range(test_df.shape[0]):  \n",
    "    x = vv[i][0]\n",
    "    ccc = test_df.iat[i,1]\n",
    "    if x > 0.5 and ccc == '2':\n",
    "        acc += 1.0\n",
    "    if x <=0.5 and ccc == '0':\n",
    "        acc += 1.0\n",
    "    dictt['p'].append(x)\n",
    "    dictt['c'].append(ccc)       \n",
    "print(acc / 198)  \n",
    "newdf = pd.DataFrame(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {'p' : [], 'c' : []}\n",
    "acc = 0.0\n",
    "for _,fn in test_df.iterrows():  \n",
    "    file_path = TRAIN_DIR + fn['filename']\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = (img/255) - 1\n",
    "    x = model.predict(tf.reshape(img,(1,224,224,3)))\n",
    "    x = x[0,0]\n",
    "    if x > 0.5 and fn['class'] == '2':\n",
    "        acc += 1.0\n",
    "    if x <=0.5 and fn['class'] == '0':\n",
    "        acc += 1.0\n",
    "    dictt['p'].append(x)\n",
    "    dictt['c'].append(fn['class'])       \n",
    "print(acc / 198)  \n",
    "newdf = pd.DataFrame(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt['p'] += 4.9\n",
    "print(dictt['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,row in test_df.iterrows():\n",
    "    print(row['filename'],row['class'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
