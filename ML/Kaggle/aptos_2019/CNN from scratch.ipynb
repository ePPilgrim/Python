{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "\n",
    "TRAIN_DIR = './data/pre/train_images/'\n",
    "TEST_DIR = './data/pre/test_images/'\n",
    "ROOT_DIR = './data/pre/'\n",
    "\n",
    "TRAIN_DF = pd.read_csv('./data/pre/train.csv')\n",
    "TEST_DF = pd.read_csv('./data/pre/test.csv')\n",
    "\n",
    "CNTS = np.array([[1443, 362],\n",
    "                 [295, 75],\n",
    "                 [799, 200],\n",
    "                 [154, 39],\n",
    "                 [235, 60]])\n",
    "\n",
    "#for i in range(100):\n",
    "#    TRAIN_DF = TRAIN_DF.sample(frac = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSingleClass(df,c,n):\n",
    "    lx = (df['class'] == c) & (df['type'] == 0)\n",
    "    res = list(df[lx]['filename'].to_numpy())\n",
    "    if lx.sum() > n:\n",
    "        return res[:n]\n",
    "    m = n - lx.sum()\n",
    "    lx = (df['class'] == c) & (df['type'] != 0)\n",
    "    lst = list(df[lx].sort_values(by='type')['filename'].to_numpy())\n",
    "    res += lst[:m]\n",
    "    return res\n",
    "    \n",
    "def GetClassesElements(df,cv,nv):\n",
    "    res = []\n",
    "    for i in range(len(cv)):\n",
    "        res += GetSingleClass(df,cv[i],nv[i])\n",
    "    return res\n",
    "        \n",
    "def GetGenerator(df,cv,nv,rootdir,bs = 32):\n",
    "    fv1 = GetClassesElements(df,cv[0],nv[0])\n",
    "    fv2 = GetClassesElements(df,cv[1],nv[1])\n",
    "    df = pd.DataFrame({'filename' : fv1 + fv2, 'class' : ['0']*len(fv1) + ['1']*len(fv2)})\n",
    "    for i in range(100):\n",
    "        df = df.sample(frac = 1).reset_index(drop=True)\n",
    "    gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "    tr_gen = gen.flow_from_dataframe(df,target_size = (IMG_SIZE, IMG_SIZE),directory = rootdir,\n",
    "                                     class_mode = 'binary', batch_size = bs) \n",
    "    return tr_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2376 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 32\n",
    "train_set = [[1],[2,3,4]]\n",
    "train_cnts = [[1188], [799,154,235]]\n",
    "train_cnt = np.array([x for y in train_cnts for x in y]).sum()\n",
    "train = GetGenerator(TRAIN_DF,train_set,train_cnts, TRAIN_DIR,bs = train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 32\n",
    "test_set = [[1],[2,3,4]]\n",
    "test_cnts = [[75], [25,25,25]]\n",
    "test_cnt = np.array([x for y in test_cnts for x in y]).sum()\n",
    "test = GetGenerator(TEST_DF,test_set,test_cnts, TEST_DIR,bs = test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 109, 109, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 454,465\n",
      "Trainable params: 454,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(256, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "#model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(256, activation = 'relu'))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 74 steps, validate for 4 steps\n",
      "Epoch 1/5\n",
      "74/74 [==============================] - 120s 2s/step - loss: 0.5799 - accuracy: 0.6766 - val_loss: 0.6803 - val_accuracy: 0.5859\n",
      "Epoch 2/5\n",
      "74/74 [==============================] - 120s 2s/step - loss: 0.5514 - accuracy: 0.7031 - val_loss: 0.7630 - val_accuracy: 0.5156\n",
      "Epoch 3/5\n",
      "74/74 [==============================] - 120s 2s/step - loss: 0.5318 - accuracy: 0.7172 - val_loss: 0.6462 - val_accuracy: 0.6250\n",
      "Epoch 4/5\n",
      "74/74 [==============================] - 120s 2s/step - loss: 0.5214 - accuracy: 0.7299 - val_loss: 0.6678 - val_accuracy: 0.5703\n",
      "Epoch 5/5\n",
      "59/74 [======================>.......] - ETA: 24s - loss: 0.5034 - accuracy: 0.7527"
     ]
    }
   ],
   "source": [
    "history = model.fit(train,\n",
    "                    steps_per_epoch = train_cnt//train_batch_size,\n",
    "                    epochs = 5,\n",
    "                    validation_data = test,\n",
    "                    validation_steps = test_cnt//test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divintosets(df, train, test, classes):\n",
    "    traindic = {'filename' : [], 'class' : []}\n",
    "    testdic = {'filename' : [], 'class' : []}\n",
    "    clsid = 0\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes[i])):\n",
    "            lx = df['diagnosis'] == classes[i][j]\n",
    "            fn = [x + '.png' for x in df['id_code'][lx].values]\n",
    "            traindic['filename'] += fn[:train[i][j]]\n",
    "            traindic['class'] += [str(clsid)] * train[i][j]\n",
    "            testdic['filename'] += fn[train[i][j] : train[i][j] + test[i][j]]\n",
    "            testdic['class'] += [str(clsid)] * test[i][j]\n",
    "        clsid += 1\n",
    "    train_df = pd.DataFrame(data = traindic)\n",
    "    test_df = pd.DataFrame(data = testdic)\n",
    "    for i in range(10):\n",
    "        train_df = train_df.sample(frac = 1).reset_index(drop=True)\n",
    "        test_df = test_df.sample(frac = 1).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = divintosets(TRAIN_DF, TRAIN_CNTS, VLD_CNTS, TRAIN_SET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range = 40,\n",
    "                                                          width_shift_range = 0.2,\n",
    "                                                          height_shift_range = 0.2,\n",
    "                                                          shear_range = 0.2,\n",
    "                                                          zoom_range = 0.2,\n",
    "                                                          fill_mode = 'nearest')\n",
    "tr_gen = tran_gen.flow_from_dataframe(train_df,\n",
    "                                      target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                      directory = TRAIN_DIR,\n",
    "                                      save_to_dir = SAVE_TO_DIR\n",
    "                                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for img in tr_gen:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = './data/temp/'\n",
    "df = pd.DataFrame({'filename' : ['xxx.png','yyy.png', 'zzz.png'], 'class' : ['7','8','9']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def dumy(img):\n",
    "    sigma = np.random.choice(25,1)[0]\n",
    "    noise = np.random.normal(0,sigma,img.shape)\n",
    "    img=cv2.addWeighted ( img.astype(float),1, noise ,1 ,0)\n",
    "    lx = img < 0\n",
    "    img[lx] = 0\n",
    "    lx = img > 255\n",
    "    img[lx] = 255\n",
    "    return img\n",
    "\n",
    "gen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range = 360,\n",
    "                                                    preprocessing_function = dumy)\n",
    "tr_gen = gen.flow_from_dataframe(df,\n",
    "                                      target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                      directory = p,\n",
    "                                      save_to_dir = p,\n",
    "                                      class_mode = 'categorical',\n",
    "                                 batch_size = 1,\n",
    "                                 shuffle = True\n",
    "                                 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255.)\n",
    "tr_gen = gen.flow_from_dataframe(train_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                 batch_size = TRAIN_BATCH_SIZE\n",
    "                                 )\n",
    "vld_gen = gen.flow_from_dataframe(test_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                  batch_size = 1,\n",
    "                                  shuffle = False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(tr_gen,\n",
    "                    steps_per_epoch = TRAIN_CNT//TRAIN_BATCH_SIZE,\n",
    "                    epochs = 3,\n",
    "                    validation_data = vld_gen,\n",
    "                    validation_steps = VLD_CNT//VLD_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = TRAIN_DF.copy()\n",
    "v = [x + '.png' for x in pred_df['id_code'].values]\n",
    "pred_df['id_code'] = v\n",
    "v = [str(x%2) for x in pred_df['diagnosis'].values]\n",
    "pred_df['diagnosis'] = v\n",
    "pred_df.columns = ['filename','class']\n",
    "pred_gen = gen.flow_from_dataframe(pred_df, \n",
    "                                 target_size = (IMG_SIZE, IMG_SIZE), \n",
    "                                 class_mode = 'binary',\n",
    "                                 directory = TRAIN_DIR,\n",
    "                                  batch_size = 64,\n",
    "                                  shuffle = False\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "vv = model.predict(pred_gen)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c1,c2 in vld_gen:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.reshape(224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR + test_df['filename'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = TRAIN_DIR + test_df['filename'][0]\n",
    "img = tf.io.read_file(file_path)\n",
    "img = tf.image.decode_png(img, channels=3)\n",
    "img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "#img = (img/255)\n",
    "#model.predict(tf.reshape(img,(1,224,224,3)))[0,0]\n",
    "#test_df['class'][0]\n",
    "print(\"min = {}, max = {}\".format(np.min(img), np.max(img)))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {'p' : [], 'c' : []}\n",
    "acc = 0.0\n",
    "for i in range(test_df.shape[0]):  \n",
    "    x = vv[i][0]\n",
    "    ccc = test_df.iat[i,1]\n",
    "    if x > 0.5 and ccc == '2':\n",
    "        acc += 1.0\n",
    "    if x <=0.5 and ccc == '0':\n",
    "        acc += 1.0\n",
    "    dictt['p'].append(x)\n",
    "    dictt['c'].append(ccc)       \n",
    "print(acc / 198)  \n",
    "newdf = pd.DataFrame(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {'p' : [], 'c' : []}\n",
    "acc = 0.0\n",
    "for _,fn in test_df.iterrows():  \n",
    "    file_path = TRAIN_DIR + fn['filename']\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = (img/255) - 1\n",
    "    x = model.predict(tf.reshape(img,(1,224,224,3)))\n",
    "    x = x[0,0]\n",
    "    if x > 0.5 and fn['class'] == '2':\n",
    "        acc += 1.0\n",
    "    if x <=0.5 and fn['class'] == '0':\n",
    "        acc += 1.0\n",
    "    dictt['p'].append(x)\n",
    "    dictt['c'].append(fn['class'])       \n",
    "print(acc / 198)  \n",
    "newdf = pd.DataFrame(dictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt['p'] += 4.9\n",
    "print(dictt['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,row in test_df.iterrows():\n",
    "    print(row['filename'],row['class'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
