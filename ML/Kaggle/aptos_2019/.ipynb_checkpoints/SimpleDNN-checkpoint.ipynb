{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "keras = tf.keras\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "IMG_SIZE = 512\n",
    "\n",
    "train_dir = './data/pre_512/train_images/'\n",
    "test_dir = './data/pre_512/test_images/'\n",
    "train_dir = './data/raw/train_images/'\n",
    "test_dir = './data/raw/test_images/'\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "lables = np.array([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=int64),\n",
       " array([1805,  370,  999,  193,  295], dtype=int64))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_df.copy()\n",
    "np.unique(df['diagnosis'].values,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumyfunc(dff, tr, val, classes):\n",
    "    lx = dff['diagnosis'].isin(classes)\n",
    "    df = dff[lx]\n",
    "    tset = []\n",
    "    vset = []\n",
    "    for i in range(df.shape[0]):\n",
    "        j = classes.index(df.iat[i,1])\n",
    "        if val[j] != 0:\n",
    "            val[j] -= 1\n",
    "            vset.append(tuple((df.iat[i,0],j)))       \n",
    "        elif tr[j] != 0:\n",
    "            tr[j] -= 1\n",
    "            tset.append(tuple((df.iat[i,0],j)))\n",
    "    return tset, vset\n",
    "\n",
    "def create_img_lab_pairs(pairs, path_dir, targetsize):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for pair in pairs:\n",
    "        file_path = path_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        imgs.append(img)\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        labels.append(cat_val)\n",
    "    imgs = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "    labels = tf.data.Dataset.from_tensor_slices(np.array(labels))\n",
    "    return tf.data.Dataset.zip((imgs,labels))\n",
    "\n",
    "def prepare_input(df,valcnts,classes,dirpath):\n",
    "    tset, vset = dumyfunc(df, valcnt, classes)\n",
    "    random.shuffle(tset)\n",
    "    tset = create_img_lab_pairs(tset, dirpath, len(classes))\n",
    "    vset = create_img_lab_pairs(vset, dirpath, len(classes))\n",
    "    return tset,vset\n",
    "\n",
    "'''def InputGenerator(sset,sz):\n",
    "    for i in range(0,len(sset),BATCH_SIZE):  \n",
    "        imgs = []\n",
    "        labels = []\n",
    "        for j in range(i,i + BATCH_SIZE):\n",
    "            pair = sset[j]\n",
    "            file_path = train_dir + pair[0] + \".png\"\n",
    "            img = tf.io.read_file(file_path)\n",
    "            img = tf.image.decode_png(img, channels=3)\n",
    "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "            img = (img/127.5) - 1\n",
    "            imgs.append(img)\n",
    "            cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "            cat_val[pair[1]] = 1\n",
    "            labels.append(cat_val)\n",
    "        yield (imgs,labels)\n",
    "        imgs = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "        labels = tf.data.Dataset.from_tensor_slices(np.array(labels))\n",
    "        yield tf.data.Dataset.zip((imgs,labels))'''\n",
    "vset = []\n",
    "tset = []\n",
    "targetsize = 2\n",
    "\n",
    "def vgen():\n",
    "    for pair in vset:  \n",
    "        file_path = train_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        cat_val = 1\n",
    "        if pair[1] == 0: \n",
    "            cat_val = 0\n",
    "        yield tuple((img,cat_val))\n",
    "\n",
    "def tgen():\n",
    "    for pair in tset:  \n",
    "        file_path = train_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        cat_val = 1\n",
    "        if pair[1] == 0: \n",
    "            cat_val = 0\n",
    "        yield tuple((img,cat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tset, vset = dumyfunc(train_df, [950,950], [49,49], [0,2])\n",
    "for i in range(1000):\n",
    "    random.shuffle(tset)\n",
    "tbatches = tf.data.Dataset.from_generator(tgen,output_types = (tf.float32, tf.int32),output_shapes = ((512, 512, 3), ())).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "vbatches = tf.data.Dataset.from_generator(vgen,output_types = (tf.float32, tf.int32),output_shapes = ((512, 512, 3), ())).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([950, 950], dtype=int64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = np.array([x[1] for x in tset])\n",
    "np.unique(rr,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, label_batch in tbatches.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 510, 510, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 255, 255, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 253, 253, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 124, 124, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 123008)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                7872576   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 7,892,033\n",
      "Trainable params: 7,892,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv11_layer = tf.keras.layers.Conv2D(16,(3,3),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "conv11_batch = conv11_layer(image_batch)\n",
    "\n",
    "conv12_layer = tf.keras.layers.Conv2D(16,(5,5),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "conv12_batch = conv12_layer(image_batch)\n",
    "\n",
    "add1_layer = tf.keras.Add()\n",
    "add1_batch = add1_layer([conv11_batch, conv12_batch])\n",
    "\n",
    "maxPool1_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool1_batch = maxPool1_layer(add1_batch)\n",
    "\n",
    "conv21_layer = tf.keras.layers.Conv2D(16,(3,3),activation = 'relu')\n",
    "conv21_batch = conv21_layer(maxPool1_batch)\n",
    "\n",
    "conv22_layer = tf.keras.layers.Conv2D(16,(5,5),activation = 'relu')\n",
    "conv22_batch = conv22_layer(maxPool1_batch)\n",
    "\n",
    "add2_layer = tf.keras.Add()\n",
    "add2_batch = add1_layer([conv21_batch, conv22_batch])\n",
    "\n",
    "maxPool2_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool2_batch = maxPool2_layer(add2_batch)\n",
    "\n",
    "conv3_layer = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu')\n",
    "conv3_batch = conv3_layer(maxPool2_batch)\n",
    "\n",
    "maxPool3_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool3_batch = maxPool3_layer(conv3_batch)\n",
    "\n",
    "'''conv4_layer = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu')\n",
    "conv4_batch = conv4_layer(maxPool3_batch)\n",
    "\n",
    "maxPool4_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool4_batch = maxPool4_layer(conv3_batch)'''\n",
    "\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "flatten_batch = flatten_layer(maxPool3_batch)\n",
    "\n",
    "dense1_layer = tf.keras.layers.Dense(64,activation = 'relu')\n",
    "dense1_batch = dense1_layer(flatten_batch)\n",
    "\n",
    "#dropout_layer2 = keras.layers.Dropout(rate = drop_out_rate)\n",
    "#dropout_batch2 = dropout_layer2(dense1_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "prediction_batch = prediction_layer(dense1_batch)\n",
    "'''conv4_layer, maxPool4_layer, conv5_layer, maxPool5_layer, flatten_layer,'''\n",
    "model = tf.keras.Sequential([conv11_layer, conv12_layer, add1_layer, maxPool1_layer,\n",
    "                             conv21_layer, conv22_layer, add2_layer, maxPool2_layer,\n",
    "                             conv3_layer, maxPool3_layer,\n",
    "                             flatten_layer, \n",
    "                             dense1_layer, \n",
    "                             prediction_layer])\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60/60 [==============================] - 507s 8s/step - loss: 0.7202 - accuracy: 0.4942 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 510s 9s/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 490s 8s/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n"
     ]
    }
   ],
   "source": [
    "initial_epochs = 5\n",
    "validation_steps=10\n",
    "history = model.fit(tbatches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=vbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_out_rate = 0.4\n",
    "\n",
    "conv1_layer = tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "conv1_batch = conv1_layer(image_batch)\n",
    "\n",
    "maxPool1_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool1_batch = maxPool1_layer(conv1_batch)\n",
    "\n",
    "conv2_layer = tf.keras.layers.Conv2D(128,(3,3),activation = 'relu')\n",
    "conv2_batch = conv2_layer(maxPool1_batch)\n",
    "\n",
    "maxPool2_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool2_batch = maxPool2_layer(conv2_batch)\n",
    "\n",
    "conv3_layer = tf.keras.layers.Conv2D(256,(3,3),activation = 'relu')\n",
    "conv3_batch = conv3_layer(maxPool2_batch)\n",
    "\n",
    "maxPool3_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool3_batch = maxPool3_layer(conv3_batch)\n",
    "\n",
    "'''conv4_layer = tf.keras.layers.Conv2D(256,(3,3),activation = 'relu')\n",
    "conv4_batch = conv4_layer(maxPool3_batch)\n",
    "\n",
    "maxPool4_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool4_batch = maxPool2_layer(conv4_batch)\n",
    "\n",
    "conv5_layer = tf.keras.layers.Conv2D(512,(3,3),activation = 'relu')\n",
    "conv5_batch = conv5_layer(maxPool4_batch)\n",
    "\n",
    "maxPool5_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool5_batch = maxPool5_layer(conv5_batch)'''\n",
    "\n",
    "average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "average_batch = average_layer(maxPool3_batch)\n",
    "\n",
    "#flatten_layer = tf.keras.layers.Flatten()\n",
    "#flatten_batch = flatten_layer(maxPool3_batch)\n",
    "\n",
    "\n",
    "\n",
    "dense1_layer = tf.keras.layers.Dense(512,activation = 'relu')\n",
    "dense1_batch = dense1_layer(average_batch)\n",
    "\n",
    "dropout_layer2 = keras.layers.Dropout(rate = drop_out_rate)\n",
    "dropout_batch2 = dropout_layer2(dense1_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(2)\n",
    "prediction_batch = prediction_layer(dropout_batch2)\n",
    "'''conv4_layer, maxPool4_layer, conv5_layer, maxPool5_layer, flatten_layer,'''\n",
    "model = tf.keras.Sequential([conv1_layer, maxPool1_layer, conv2_layer, maxPool2_layer, conv3_layer, maxPool3_layer,\n",
    "                             average_layer,\n",
    "                             dense1_layer, dropout_layer2, prediction_layer])\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses. CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "'''model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])'''\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "validation_steps=20\n",
    "history = model.fit(tbatches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=vbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
