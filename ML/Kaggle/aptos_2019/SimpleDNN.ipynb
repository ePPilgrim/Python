{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "keras = tf.keras\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "IMG_SIZE = 512\n",
    "\n",
    "train_dir = './data/pre_512/train_images/'\n",
    "test_dir = './data/pre_512/test_images/'\n",
    "#train_dir = './data/raw/train_images/'\n",
    "#test_dir = './data/raw/test_images/'\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "lables = np.array([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.copy()\n",
    "np.unique(df['diagnosis'].values,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumyfunc(dff, tr, val, classes):\n",
    "    lx = dff['diagnosis'].isin(classes)\n",
    "    df = dff[lx]\n",
    "    tset = []\n",
    "    vset = []\n",
    "    for i in range(df.shape[0]):\n",
    "        j = classes.index(df.iat[i,1])\n",
    "        if val[j] != 0:\n",
    "            val[j] -= 1\n",
    "            vset.append(tuple((df.iat[i,0],j)))       \n",
    "        elif tr[j] != 0:\n",
    "            tr[j] -= 1\n",
    "            tset.append(tuple((df.iat[i,0],j)))\n",
    "    return tset, vset\n",
    "\n",
    "def create_img_lab_pairs(pairs, path_dir, targetsize):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for pair in pairs:\n",
    "        file_path = path_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        #img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        imgs.append(img)\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        labels.append(cat_val)\n",
    "    imgs = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "    labels = tf.data.Dataset.from_tensor_slices(np.array(labels))\n",
    "    return tf.data.Dataset.zip((imgs,labels))\n",
    "\n",
    "def prepare_input(df,valcnts,classes,dirpath):\n",
    "    tset, vset = dumyfunc(df, valcnt, classes)\n",
    "    random.shuffle(tset)\n",
    "    tset = create_img_lab_pairs(tset, dirpath, len(classes))\n",
    "    vset = create_img_lab_pairs(vset, dirpath, len(classes))\n",
    "    return tset,vset\n",
    "\n",
    "'''def InputGenerator(sset,sz):\n",
    "    for i in range(0,len(sset),BATCH_SIZE):  \n",
    "        imgs = []\n",
    "        labels = []\n",
    "        for j in range(i,i + BATCH_SIZE):\n",
    "            pair = sset[j]\n",
    "            file_path = train_dir + pair[0] + \".png\"\n",
    "            img = tf.io.read_file(file_path)\n",
    "            img = tf.image.decode_png(img, channels=3)\n",
    "            img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "            img = (img/127.5) - 1\n",
    "            imgs.append(img)\n",
    "            cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "            cat_val[pair[1]] = 1\n",
    "            labels.append(cat_val)\n",
    "        yield (imgs,labels)\n",
    "        imgs = tf.data.Dataset.from_tensor_slices(imgs)\n",
    "        labels = tf.data.Dataset.from_tensor_slices(np.array(labels))\n",
    "        yield tf.data.Dataset.zip((imgs,labels))'''\n",
    "vset = []\n",
    "tset = []\n",
    "targetsize = 2\n",
    "\n",
    "def vgen():\n",
    "    for pair in vset:  \n",
    "        file_path = train_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        #img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        cat_val = 1\n",
    "        if pair[1] == 0: \n",
    "            cat_val = 0\n",
    "        yield tuple((img,cat_val))\n",
    "\n",
    "def tgen():\n",
    "    for pair in tset:  \n",
    "        file_path = train_dir + pair[0] + \".png\"\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = (img/127.5) - 1\n",
    "        #img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        cat_val = np.zeros(targetsize,dtype = 'int')\n",
    "        cat_val[pair[1]] = 1\n",
    "        cat_val = 1\n",
    "        if pair[1] == 0: \n",
    "            cat_val = 0\n",
    "        yield tuple((img,cat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tset, vset = dumyfunc(train_df, [950,950], [49,49], [0,2])\n",
    "for i in range(1000):\n",
    "    random.shuffle(tset)\n",
    "tbatches = tf.data.Dataset.from_generator(tgen,output_types = (tf.float32, tf.int32),output_shapes = ((512, 512, 3), ())).shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "vbatches = tf.data.Dataset.from_generator(vgen,output_types = (tf.float32, tf.int32),output_shapes = ((512, 512, 3), ())).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = np.array([x[1] for x in tset])\n",
    "np.unique(rr,return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for image_batch, label_batch in tbatches.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_batch = tf.keras.Input(shape = (IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "conv11_layer = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3),padding = \"same\")\n",
    "conv11_batch = conv11_layer(input_batch)\n",
    "\n",
    "conv12_layer = tf.keras.layers.Conv2D(32,(5,5),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3),padding = \"same\")\n",
    "conv12_batch = conv12_layer(input_batch)\n",
    "\n",
    "add1_layer = tf.keras.layers.Concatenate()\n",
    "add1_batch = add1_layer([conv11_batch, conv12_batch])\n",
    "\n",
    "maxPool1_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool1_batch = maxPool1_layer(add1_batch)\n",
    "\n",
    "conv21_layer = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',padding = \"same\")\n",
    "conv21_batch = conv21_layer(maxPool1_batch)\n",
    "\n",
    "conv22_layer = tf.keras.layers.Conv2D(32,(5,5),activation = 'relu',padding = \"same\")\n",
    "conv22_batch = conv22_layer(maxPool1_batch)\n",
    "\n",
    "add2_layer = tf.keras.layers.Concatenate()\n",
    "add2_batch = add1_layer([conv21_batch, conv22_batch])\n",
    "\n",
    "maxPool2_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool2_batch = maxPool2_layer(add2_batch)\n",
    "\n",
    "conv3_layer = tf.keras.layers.Conv2D(128,(3,3),activation = 'relu')\n",
    "conv3_batch = conv3_layer(maxPool2_batch)\n",
    "\n",
    "maxPool3_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool3_batch = maxPool3_layer(conv3_batch)\n",
    "\n",
    "conv4_layer = tf.keras.layers.Conv2D(128,(3,3),activation = 'relu')\n",
    "conv4_batch = conv4_layer(maxPool3_batch)\n",
    "\n",
    "maxPool4_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool4_batch = maxPool4_layer(conv4_batch)\n",
    "\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "flatten_batch = flatten_layer(maxPool3_batch)\n",
    "\n",
    "dense1_layer = tf.keras.layers.Dense(64,activation = 'relu')\n",
    "dense1_batch = dense1_layer(flatten_batch)\n",
    "\n",
    "#dropout_layer2 = keras.layers.Dropout(rate = drop_out_rate)\n",
    "#dropout_batch2 = dropout_layer2(dense1_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "prediction_batch = prediction_layer(dense1_batch)\n",
    "model = tf.keras.Model(inputs = input_batch, outputs = prediction_batch)\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "validation_steps=10\n",
    "history = model.fit(tbatches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=vbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_out_rate = 0.4\n",
    "\n",
    "conv1_layer = tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "conv1_batch = conv1_layer(image_batch)\n",
    "\n",
    "maxPool1_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool1_batch = maxPool1_layer(conv1_batch)\n",
    "\n",
    "conv2_layer = tf.keras.layers.Conv2D(128,(3,3),activation = 'relu')\n",
    "conv2_batch = conv2_layer(maxPool1_batch)\n",
    "\n",
    "maxPool2_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool2_batch = maxPool2_layer(conv2_batch)\n",
    "\n",
    "conv3_layer = tf.keras.layers.Conv2D(256,(3,3),activation = 'relu')\n",
    "conv3_batch = conv3_layer(maxPool2_batch)\n",
    "\n",
    "maxPool3_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool3_batch = maxPool3_layer(conv3_batch)\n",
    "\n",
    "'''conv4_layer = tf.keras.layers.Conv2D(256,(3,3),activation = 'relu')\n",
    "conv4_batch = conv4_layer(maxPool3_batch)\n",
    "\n",
    "maxPool4_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool4_batch = maxPool2_layer(conv4_batch)\n",
    "\n",
    "conv5_layer = tf.keras.layers.Conv2D(512,(3,3),activation = 'relu')\n",
    "conv5_batch = conv5_layer(maxPool4_batch)\n",
    "\n",
    "maxPool5_layer = tf.keras.layers.MaxPooling2D((2,2))\n",
    "maxPool5_batch = maxPool5_layer(conv5_batch)'''\n",
    "\n",
    "average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "average_batch = average_layer(maxPool3_batch)\n",
    "\n",
    "#flatten_layer = tf.keras.layers.Flatten()\n",
    "#flatten_batch = flatten_layer(maxPool3_batch)\n",
    "\n",
    "\n",
    "\n",
    "dense1_layer = tf.keras.layers.Dense(512,activation = 'relu')\n",
    "dense1_batch = dense1_layer(average_batch)\n",
    "\n",
    "dropout_layer2 = keras.layers.Dropout(rate = drop_out_rate)\n",
    "dropout_batch2 = dropout_layer2(dense1_batch)\n",
    "\n",
    "prediction_layer = tf.keras.layers.Dense(2)\n",
    "prediction_batch = prediction_layer(dropout_batch2)\n",
    "'''conv4_layer, maxPool4_layer, conv5_layer, maxPool5_layer, flatten_layer,'''\n",
    "model = tf.keras.Sequential([conv1_layer, maxPool1_layer, conv2_layer, maxPool2_layer, conv3_layer, maxPool3_layer,\n",
    "                             average_layer,\n",
    "                             dense1_layer, dropout_layer2, prediction_layer])\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses. CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "'''model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])'''\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 5\n",
    "validation_steps=20\n",
    "history = model.fit(tbatches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=vbatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
