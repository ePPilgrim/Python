{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LassoLars,  BayesianRidge, LassoLarsIC, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "project_dir = 'C:\\\\Users\\\\PLDD\\\\python\\\\Python\\\\ML\\\\Kaggle\\\\house_price'\n",
    "raw_path = os.path.join(project_dir,'data','raw')\n",
    "train_path = os.path.join(raw_path, 'train.csv')\n",
    "test_path = os.path.join(raw_path, 'test.csv')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainTestSets(train_path, test_path):   \n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    col = ['FireplaceQu','Alley','PoolQC','Fence','MiscFeature']\n",
    "    train[col] = train[col].fillna(value = 'None')\n",
    "    test[col] = test[col].fillna(value = 'None')\n",
    "    \n",
    "    catcol = [['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'],\n",
    "              ['GarageType','GarageFinish','GarageQual','GarageCond']]\n",
    "    numcol = [['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath'],\n",
    "              ['GarageYrBlt','GarageCars','GarageArea']]\n",
    "    keyflds = ['TotalBsmtSF','GarageArea']\n",
    "    for (catflds,numflds,keyfld) in zip(catcol, numcol, keyflds):\n",
    "        lx = (train[keyfld] == 0) & (train[catflds].isnull().any(axis = 1))\n",
    "        train.loc[lx,catflds] = train[lx][catflds].fillna(value = 'None')\n",
    "        lx = (train[keyfld] == 0) & (train[numflds].isnull().any(axis = 1))\n",
    "        train.loc[lx,numflds] = train[lx][numflds].fillna(value = 0)\n",
    "        lx = (test[keyfld] == 0) & (test[catflds].isnull().any(axis = 1))\n",
    "        test.loc[lx,catflds] = test[lx][catflds].fillna(value = 'None')\n",
    "        lx = (test[keyfld] == 0) & (test[numflds].isnull().any(axis = 1))\n",
    "        test.loc[lx,numflds] = test[lx][numflds].fillna(value = 0)\n",
    "    train = train.drop('Utilities', axis = 1)\n",
    "    test = test.drop('Utilities', axis = 1)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessOutliers(train, test):\n",
    "    lx1 = (train['SalePrice'] <= 200000) & (train['GrLivArea'] >= 4000) \n",
    "    lx2 = train['LotFrontage'] > 300 \n",
    "    lx = lx1 | lx2\n",
    "    train = train[~lx]\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFeature(train, test):\n",
    "    #train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRegresion(df,fld):\n",
    "    scaler = QuantileTransformer()\n",
    "    param_grid = {'alpha': [1e0,0.5, 0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6],'gamma': np.logspace(-4, 2, 14)}\n",
    "    solver = GridSearchCV(KernelRidge(kernel = 'rbf'), cv=5, param_grid=param_grid)\n",
    "    #solver = linear_model.RidgeCV(alphas = np.arange(0.001, 5, 0.001))\n",
    "    \n",
    "    #param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6,1e-7]}\n",
    "    #solver = GridSearchCV(Ridge(max_iter = 10000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "    #scaler_x = RobustScaler() #make_pipeline(RobustScaler(),PowerTransformer())\n",
    "    #scaler_y = PowerTransformer()\n",
    "    scaler_x = QuantileTransformer() \n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    lx = df[fld].isnull()\n",
    "    dfy = df[fld]\n",
    "    dfx = df.drop(fld, axis = 1)\n",
    "    X = dfx[~lx].as_matrix().astype('float')\n",
    "    y = dfy[~lx].ravel()\n",
    "    #scaler_y = scaler_y.fit(y.reshape(-1,1))\n",
    "    #y = scaler_y.transform(y.reshape(-1,1)).ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "    scaler_x = scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "\n",
    "    solver.fit(X_train, y_train)\n",
    "    print(solver.score(X_test, y_test))\n",
    "    X = scaler_x.fit_transform(X)\n",
    "    XX = scaler_x.transform(dfx[lx].as_matrix().astype('float'))\n",
    "    solver.fit(X, y) \n",
    "    yy = solver.predict(XX).astype('float')\n",
    "    dfy[lx] = yy#scaler_y.inverse_transform(yy.reshape(-1,1)).ravel()\n",
    "    return dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessAll(df):\n",
    "    df['MSSubClass'] = df['MSSubClass'].apply(str)\n",
    "    df['OverallCond'] = df['OverallCond'].astype(str)\n",
    "    df['YrSold'] = df['YrSold'].astype(str)\n",
    "    df['MoSold'] = df['MoSold'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessNumNans(df):\n",
    "    fld = 'LotFrontage'\n",
    "    df.loc[:,[fld]] = FindRegresion(df[['LotFrontage','Neighborhood','LotConfig','LotArea']],fld)\n",
    "    \n",
    "    lx = (df['MasVnrArea'] != 0) & (df['MasVnrType'] == 'None')\n",
    "    df.loc[lx,'MasVnrArea'] = 0.0\n",
    "    lx = (df['MasVnrArea'] == 0) & (df['MasVnrType'] != 'None')\n",
    "    df.loc[lx,'MasVnrType'] = 'None'\n",
    "    \n",
    "    numflds = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2']\n",
    "    catflds = ['MasVnrType', 'BsmtFinType1','BsmtFinType2']\n",
    "\n",
    "    for (catfld, numfld) in zip(catflds,numflds):\n",
    "        lx = df[numfld].isnull()\n",
    "        tmp = df.groupby([catfld])[numfld].describe()\n",
    "        df.loc[lx,numfld] = np.sum(tmp['50%'] * tmp['count'] / tmp['count'].sum())   \n",
    "        \n",
    "    fld = 'TotalBsmtSF'\n",
    "    lx = df[fld].isnull()\n",
    "    df.loc[lx,fld] = df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "    \n",
    "    fld = 'BsmtUnfSF'\n",
    "    lx = df[fld].isnull()\n",
    "    df.loc[lx,fld] = df[lx]['TotalBsmtSF'] - df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "    \n",
    "    flds = ['BsmtFullBath','BsmtHalfBath'] \n",
    "    lx = df['BsmtFullBath'].isnull()\n",
    "    df.loc[lx,['BsmtFullBath','BsmtHalfBath']]= df[~lx][['BsmtFullBath','BsmtHalfBath']].mean(axis = 0).values\n",
    "    \n",
    "    lx = df['GarageYrBlt'].isnull()\n",
    "    df.loc[lx,'GarageYrBlt'] = df[lx]['YearRemodAdd']\n",
    "    lx = df['GarageCars'].isnull()\n",
    "    df.loc[lx,'GarageCars'] = df[~lx]['GarageCars'].mean()\n",
    "    lx = df['GarageArea'].isnull()\n",
    "    df.loc[lx,'GarageArea'] = df[~lx]['GarageArea'].median()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessCategoricalNans(df):\n",
    "    oldflds = []\n",
    "    newdf = pd.DataFrame()\n",
    "######################## 1) MSZoning - 'RL', 'RM', 'C (all)', 'FV', 'RH', nan ############################\n",
    "    colname = 'MSZoning'\n",
    "    oldflds.append(colname)\n",
    "    #### with Mitchel neighbor it is highly likely that zoning is 'RL' (grouping by Neighbors + BuiltYear)\n",
    "    lx=(df[colname].isnull()) & (df['Neighborhood'] == 'Mitchel')  \n",
    "    df.loc[lx,colname] = 'RL'\n",
    "    ### group by IDOTRR neighbor\n",
    "    invalidlx = df[colname].isnull() \n",
    "    idotrrlx = df['Neighborhood'] == 'IDOTRR'\n",
    "\n",
    "    tempdf = pd.get_dummies(df[colname],prefix = colname).astype(float)\n",
    "    tempdf.loc[invalidlx,:] = tempdf[idotrrlx].sum().values / tempdf[idotrrlx].sum().sum()\n",
    "    newdf[tempdf.columns] = tempdf\n",
    "######################## 2) ordered categorias\n",
    "    catflds = ['FireplaceQu','ExterQual','ExterCond','HeatingQC','PoolQC','LandSlope','LotShape',\n",
    "               'PavedDrive','Street','Alley','CentralAir','MSSubClass','OverallCond','YrSold','MoSold']\n",
    "    df['FireplaceQu'] = pd.Categorical(df['FireplaceQu'].values,  ['None','Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['ExterQual'] = pd.Categorical(df['ExterQual'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['ExterCond'] = pd.Categorical(df['ExterCond'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['HeatingQC'] = pd.Categorical(df['HeatingQC'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['PoolQC'] = pd.Categorical(df['PoolQC'].values, ['None', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['LandSlope'] = pd.Categorical(df['LandSlope'].values, ['Sev', 'Mod', 'Gtl'], ordered = True)\n",
    "    df['LotShape'] = pd.Categorical(df['LotShape'].values, ['Reg', 'IR1', 'IR2', 'IR3'], ordered = True)\n",
    "    df['PavedDrive'] = pd.Categorical(df['PavedDrive'].values, ['N', 'P', 'Y'], ordered = True)\n",
    "    df['Street'] = pd.Categorical(df['Street'].values, ['Grvl','Pave'], ordered = True)\n",
    "    df['Alley'] = pd.Categorical(df['Alley'].values, ['None', 'Grvl', 'Pave'], ordered = True)\n",
    "    df['CentralAir'] = pd.Categorical(df['CentralAir'].values, ['N', 'Y'], ordered = True)\n",
    "    df['MSSubClass'] = pd.Categorical(df['MSSubClass'].values, ['20','30','40','45','50','60','70','75','80','85','90','120','150','160','180','190'], ordered = True)\n",
    "    df['OverallCond'] = pd.Categorical(df['OverallCond'].values, ['1','2','3','4','5','6','7','8','9'], ordered = True)\n",
    "    df['YrSold'] = pd.Categorical(df['YrSold'].values, ['2006','2007','2008','2009','2010'], ordered = True)\n",
    "    df['MoSold'] = pd.Categorical(df['MoSold'].values, ['1','2','3','4','5','6','7','8','9','10','11','12'], ordered = True)\n",
    "\n",
    "    nanflds = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','KitchenQual',\n",
    "               'GarageFinish', 'GarageQual','GarageCond']\n",
    "    df['BsmtQual'] = pd.Categorical(df['BsmtQual'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['BsmtCond'] = pd.Categorical(df['BsmtCond'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['BsmtExposure'] = pd.Categorical(df['BsmtExposure'].values, ['None', 'No', 'Mn', 'Av', 'Gd'], ordered = True)\n",
    "    df['BsmtFinType1'] = pd.Categorical(df['BsmtFinType1'].values, ['None','Unf','LwQ', 'Rec','BLQ','ALQ','GLQ'], ordered = True)\n",
    "    df['BsmtFinType2'] = pd.Categorical(df['BsmtFinType2'].values, ['None','Unf','LwQ', 'Rec','BLQ','ALQ','GLQ'], ordered = True)\n",
    "    df['KitchenQual'] = pd.Categorical(df['KitchenQual'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['GarageFinish'] = pd.Categorical(df['GarageFinish'].values, ['None', 'Unf', 'RFn', 'Fin'], ordered = True)\n",
    "    df['GarageQual'] = pd.Categorical(df['GarageQual'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['GarageCond'] = pd.Categorical(df['GarageCond'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df.loc[:,catflds + nanflds] = df[catflds + nanflds].apply(lambda x: x.cat.codes).astype('float')\n",
    "    for fld in nanflds:\n",
    "        lx = df[fld] < 0.0\n",
    "        df.loc[lx,fld] = df[~lx][fld].mean()\n",
    "######################## 3) 'Exterior1st', 'Exterior2nd','MasVnrType','Electrical','KitchenQual'#############\n",
    "########################'BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'##################\n",
    "######################## 'Functional','GarageType', 'GarageFinish', 'GarageQual','GarageCond'################\n",
    "######################## 'PoolQC','SaleType'#################################################################   \n",
    "    catflds = ['Exterior1st','Exterior2nd','MasVnrType','Electrical','GarageType','SaleType','Functional']\n",
    "    for fld in catflds:\n",
    "        lx = df[fld].isnull()\n",
    "        oldflds.append(fld)\n",
    "        tmp = pd.get_dummies(df[fld],prefix = fld).astype(float)\n",
    "        tmp.loc[lx,:] = tmp[~lx].sum().values / tmp[~lx].sum().sum()\n",
    "        newdf[tmp.columns] = tmp\n",
    "    df = df.drop(labels = oldflds,axis = 1)\n",
    "    df[newdf.columns] = newdf\n",
    "    df = pd.get_dummies(df).astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormatAndSave(df):\n",
    "    processed_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_train_path = os.path.join(processed_data_path, 'train.csv')\n",
    "    write_test_path = os.path.join(processed_data_path, 'test.csv')\n",
    "    train_df = df[df['SalePrice'] != 0]\n",
    "    train_df.loc[:,'Id'] = train_df['Id'].astype('int32').values\n",
    "    test_df = df[df['SalePrice'] == 0]\n",
    "    test_df = test_df.drop(labels = 'SalePrice',axis = 1)\n",
    "    test_df.loc[:,'Id'] = test_df['Id'].astype('int32').values\n",
    "    train_df.to_csv(write_train_path)\n",
    "    test_df.to_csv(write_test_path) \n",
    "    return (df,train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetResultData(train_path, test_path):\n",
    "    (train, test) = GetTrainTestSets(train_path, test_path)\n",
    "    (train, test) = ProccessOutliers(train, test)\n",
    "    (train, test) = ProcessFeature(train, test)\n",
    "    train1 = train\n",
    "    test1 = test\n",
    "    test['SalePrice'] = 0\n",
    "    return (train.append(test), train1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLDD\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735878919174737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLDD\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\PLDD\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:601: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "(df, train, test) = GetResultData(train_path, test_path)\n",
    "#df = ProccessNumNans(df)\n",
    "(df, train, test) = df.pipe(PreprocessAll).pipe(ProccessNumNans).pipe(ProccessCategoricalNans).pipe(FormatAndSave)\n",
    "#best score = 0.627"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "train_path = os.path.join(processed_data_path, 'train.csv')\n",
    "test_path = os.path.join(processed_data_path, 'test.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df = test_df.drop(labels = ['Unnamed: 0'],axis = 1)\n",
    "train_df = train_df.drop(labels = ['Id','Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_ml_solve(train_df, test_df, scaler, solver, q = 0.4, target = 'temp.csv'):\n",
    "    collx = train_df.columns != 'SalePrice'\n",
    "    X = train_df.loc[:,collx ].as_matrix().astype('float')\n",
    "    y = train_df['SalePrice'].ravel()\n",
    "    scaler_y = RobustScaler()#PowerTransformer()\n",
    "    scaler_y = scaler_y.fit(y.reshape(-1,1))\n",
    "    #y = scaler_y.transform(y.reshape(-1,1))\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=q, random_state=0)\n",
    "    #scaler = scaler.fit(X_train)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    sscore = 0.0\n",
    "    if q != 0:\n",
    "        X_test = scaler.transform(X_test)\n",
    "        solver.fit(X_train, y_train)\n",
    "        sscore = solver.score(X_test, y_test)\n",
    "    else: \n",
    "        print(\"No test subsets!!!!\")\n",
    "    scaler = scaler.fit(X) \n",
    "    X = scaler.transform(X)  \n",
    "    ids = test_df['Id'].values\n",
    "    test_df = test_df.drop(labels = 'Id',axis = 1)\n",
    "    XX = scaler.transform(test_df.as_matrix().astype('float'))\n",
    "    solver.fit(X, y) \n",
    "    pred_y = solver.predict(XX).astype('float')\n",
    "    df_submission = pd.DataFrame({'Id': ids, 'SalePrice' : np.exp(pred_y) - 1.0} )\n",
    "    #df_submission = pd.DataFrame({'Id': ids, 'SalePrice' : scaler_y.inverse_transform(pred_y.reshape(-1,1)).ravel()} )\n",
    "    submission_data_path = os.path.join(os.path.pardir,'data','external')\n",
    "    submission_file_path = os.path.join(submission_data_path, target)\n",
    "    df_submission.to_csv(submission_file_path, index=False)\n",
    "    print(sscore)\n",
    "    return (df_submission, solver, sscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = RobustScaler()\n",
    "scaler4 = Normalizer()\n",
    "scaler5 = QuantileTransformer()\n",
    "scaler6 = PowerTransformer()\n",
    "scaler7 = make_pipeline(RobustScaler(), PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLDD\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9156653808926848\n",
      "{'alpha': 7.0}\n",
      "0.9163517290520821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLDD\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Ridge Linear Regresion\n",
    "param_grid = {'alpha': [7.0,6.0,5.0,3.0,2.0,1.5,1.3,1.2,1.1,1.001,0.8,0.5,0.2,0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6,1e-7]}\n",
    "solver = GridSearchCV(Ridge(max_iter = 10000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "scaler = RobustScaler()\n",
    "(ridgedm, ridgesolver, ridgescore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'Ridged.csv')\n",
    "print(ridgesolver.best_params_)\n",
    "print(ridgesolver.best_score_)\n",
    "#max score = 0.9158041549014316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183152160821689\n"
     ]
    }
   ],
   "source": [
    "#Lasso Linear Regresion\n",
    "#param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7]}\n",
    "param_grid = {'alpha': [ 0.04,0.03, 0.02,0.001, 0.000888889, 0.0008]}\n",
    "solver = GridSearchCV(Lasso(max_iter = 25000,tol = 0.000001), cv=5,param_grid=param_grid)\n",
    "solver = Lasso(max_iter = 25000,tol = 0.000001, alpha = 0.000888889)\n",
    "scaler = RobustScaler()\n",
    "(lassom, lassomsolver, lassomscore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'Lasso.csv')\n",
    "#print(lassomsolver.best_params_)\n",
    "#print(lassomsolver.best_score_)\n",
    "#max score = 0.9183107282670468\n",
    "#0.00089 0.9183148121017135\n",
    "#0.000889 0.9183151760580683\n",
    "#alpha = 0.000888889 score = 0.9183152160821689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9183152193227974\n",
      "{'alpha': 0.00088888}\n",
      "0.9193322693422967\n"
     ]
    }
   ],
   "source": [
    "#LassoLARS Linear Regresion\n",
    "param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7]}\n",
    "param_grid = {'alpha': [0.001, 0.00088888]}\n",
    "solver = GridSearchCV(Lasso(max_iter = 25000,tol = 0.000001), cv=5,param_grid=param_grid)\n",
    "#solver = LassoLars(max_iter = 25000,alpha = 0.00088888)\n",
    "scaler = RobustScaler()\n",
    "(lassolarsm, lassolarsmsolver, lassolarsmscore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'LassoLars.csv')\n",
    "print(lassolarsmsolver.best_params_)\n",
    "print(lassolarsmsolver.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoLarsIC Linear Regresion\n",
    "solver = LassoLarsIC(max_iter = 25000, criterion = 'aic')\n",
    "scaler = RobustScaler()\n",
    "lassolarsICm = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'LassoLarsIC.csv')\n",
    "#max score = 0.8964009621989726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in np.logspace(start = 0.0001, stop = 0.99, num=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Regresion\n",
    "param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.99,0.9,0.8,0.6,0.5,0.4,0.3,0.25,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7],\n",
    "             'l1_ratio' : [1e-3,5e-3,1e-2,5e-2,0.1,0.2,0.25,0.3,0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.901,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,0.995,0.999]}\n",
    "solver = GridSearchCV(ElasticNet(max_iter = 25000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "print(solver.estimator.alpha)\n",
    "scaler = RobustScaler()\n",
    "elastic_net_m = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'ElasticNet.csv')\n",
    "#max_score = 0.9183288770325282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver1 = linear_model.RidgeCV(alphas = np.arange(0.001, 5, 0.001))\n",
    "solver2 = linear_model.LassoCV()\n",
    "solver3 = linear_model.ElasticNetCV(alphas = np.arange(0.001, 5, 0.001), max_iter = 20000 )\n",
    "solver4 = linear_model.BayesianRidge()\n",
    "solver6 = linear_model.PassiveAggressiveRegressor(C = 10,tol = 0.00001, max_iter = 50000)\n",
    "solver7 = linear_model.HuberRegressor(max_iter=10000)\n",
    "solver8 = Pipeline([('poly', PolynomialFeatures(degree=2)),  ('linear', solver1)])\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [1e0,0.5, 0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6],\n",
    "    'gamma': np.logspace(-4, 2, 14)\n",
    "}\n",
    "solver5_1 = GridSearchCV(KernelRidge(kernel = 'polynomial',degree = 80), cv=8, param_grid=param_grid)\n",
    "solver5_2 = GridSearchCV(KernelRidge(kernel = 'rbf'), cv=8, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subm = linear_ml_solve(train_df, test_df, scaler5, solver2, 0.4,'11_LassoCV.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
