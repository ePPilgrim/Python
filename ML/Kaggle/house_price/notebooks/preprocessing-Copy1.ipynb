{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as npma\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing.data import QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LassoLars,  BayesianRidge, LassoLarsIC, Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "project_dir ='C:\\\\Users\\\\PLDD\\\\python\\\\Python\\\\ML\\\\Kaggle\\\\house_price'\n",
    "#project_dir = 'C:\\\\Users\\\\PLDD\\\\Practice\\\\Projects\\\\Python\\\\ML\\\\Kaggle\\\\house_price'\n",
    "raw_path = os.path.join(project_dir,'data','raw')\n",
    "train_path = os.path.join(raw_path, 'train.csv')\n",
    "test_path = os.path.join(raw_path, 'test.csv')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "test['SalePrice'] = 0.0\n",
    "df = train.append(test)\n",
    "\n",
    "class BaseTransformer:\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "class Transformer1(BaseTransformer):\n",
    "    def __init__(self, aKeyfld, aSets):\n",
    "        self.sets = aSets\n",
    "        self.keyfld = aKeyfld\n",
    "    def transform(self, X):\n",
    "        lx = X.isnull().values & (X[self.keyfld] == 0).values.reshape(-1,1)\n",
    "        for key in self.sets:\n",
    "            masklx = lx & np.array([[el in self.sets[key] for el in X.columns]]).reshape(1,-1)\n",
    "            X = X.mask(masklx, other = key )\n",
    "        return X\n",
    "class Transformer2(BaseTransformer):\n",
    "    def __init__(self,aMeanFlds, aMedianFlds):\n",
    "        self.meanflds = aMeanFlds\n",
    "        self.medianflds = aMedianFlds\n",
    "    def transform(self, X):\n",
    "        concv = X[self.meanflds].mean(axis = 0).append(X[self.medianflds].median(axis = 0))\n",
    "        concf = self.meanflds + self.medianflds\n",
    "        lx = X[concf].isnull().values\n",
    "        X[concf] = X[concf].mask(lx,np.broadcast_to(concv,lx.shape))\n",
    "        return X\n",
    "class Transformer3(BaseTransformer):\n",
    "    def __init__(self,aFromFlds, aToFlds):\n",
    "        self.fromflds = aFromFlds\n",
    "        self.toflds = aToFlds\n",
    "    def transform(self,X):\n",
    "        lx = X[self.toflds].isnull().values\n",
    "        X[self.toflds] = X[self.toflds].mask(lx, X[self.fromflds].values)\n",
    "        return X\n",
    "class Transformer4(BaseTransformer):  \n",
    "    def __init__(self,aResFlds, aXFlds, aYFlds, aDropFlds):\n",
    "        self.zflds = aResFlds\n",
    "        self.xflds = aXFlds\n",
    "        self.yflds = aYFlds\n",
    "        self.dropflds = aDropFlds\n",
    "    def transform(self,X):\n",
    "        X[self.yflds] = X[self.xflds] - X[self.yflds].values\n",
    "        X.columns = [self.zflds[self.yflds.index(el)] if el in self.yflds else el for el in X.columns]\n",
    "        X = X.drop(columns = self.dropflds)\n",
    "        return X\n",
    "    \n",
    "class Transformer5(BaseTransformer):  \n",
    "    def __init__(self):\n",
    "        'None','Po', 'Fa', 'TA', 'Gd', 'Ex'\n",
    "        self.sortmap = {'None':['Po','Grvl','No','Unf'],'Po':'Fa','Fa':'TA','TA':'Gd','Gd':['Ex',None],'Ex':None,\n",
    "                       'Sev':'Mod','Mod':'Gtl','Gtl':None,\n",
    "                       'Reg':'IR1','IR1':'IR2','IR2':'IR3','IR3':None,\n",
    "                       'N':'P','P':'Y','Y':None,\n",
    "                       'Grvl':'Pave','Pave':None,\n",
    "                       'No':'Mn','Mn':'Av','Av':'Gd',\n",
    "                       'Unf':['LwQ','RFn'],'LwQ':'Rec','Rec':'BLQ','BLQ':'ALQ','ALQ':'GLQ','GLQ':None,\n",
    "                       'RFn':'Fin','Fin':None}\n",
    "    def __sort_fields(self,flds):\n",
    "        \n",
    "        \n",
    "    def transform(self,X):\n",
    "          \n",
    "    \n",
    "    \n",
    "    #lx = df['GarageYrBlt'].isnull()\n",
    "    #df.loc[lx,'GarageYrBlt'] = df[lx]['YearRemodAdd']\n",
    "    #lx = df['GarageCars'].isnull()\n",
    "    #df.loc[lx,'GarageCars'] = df[~lx]['GarageCars'].mean()\n",
    "    #lx = df['GarageArea'].isnull()\n",
    "    #df.loc[lx,'GarageArea'] = df[~lx]['GarageArea'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garage + Year\n",
    "f = ['GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond',\n",
    "     'YearBuilt','YearRemodAdd','MoSold','YrSold']  \n",
    "sets = {0 : ['GarageYrBlt','GarageCars'],\n",
    "        'None' : ['GarageType','GarageFinish','GarageQual','GarageCond']}\n",
    "ml = df[f].copy()\n",
    "tr = make_pipeline(Transformer1('GarageArea',{0 : ['GarageYrBlt','GarageCars'],\n",
    "                                              'None' : ['GarageType','GarageFinish','GarageQual','GarageCond']}),\n",
    "                   Transformer2(['GarageCars'],['GarageArea']),\n",
    "                   Transformer3(['YearRemodAdd'],['GarageYrBlt']),\n",
    "                   Transformer4(['BuiltAge','RenowateAge','GarageAge'],\n",
    "                                ['YrSold']*3,['YearBuilt','YearRemodAdd','GarageYrBlt'],['MoSold']))\n",
    "ml = tr.transform(ml)\n",
    "ml.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ['aaa','bbb', None]\n",
    "c = 'rrrrr'\n",
    "#c = v\n",
    "w = c if type(c) is list else [c]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ['1','2','3','4','5']\n",
    "s2 = ['2', '4']\n",
    "s = ['222', '444']\n",
    "[s[s2.index(el)] if el in s2 else el for el in s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "dd = pd.DataFrame({'a' : [1,2,1], 'b' : [3,1,22], 'c' : [44,66,41], 'd' : ['a','e','a']})\n",
    "dd1 = dd.copy()\n",
    "ww = pd.DataFrame(data = (dd[['a','a']] - dd[['b','b']].values).values,columns = ['t1','t2'])\n",
    "\n",
    "dd[['b','c']] =dd[['a','a']] - dd[['b','c']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "v=np.array([1,2,3]).reshape(1,-1)\n",
    "lx = np.array([[True, False, True], [False, True, False], [False, False, False], [True, True, True]])\n",
    "bb = np.zeros((4,3))\n",
    "bb[lx] = np.broadcast_to(v,(lx.shape[0],v.shape[1]))[lx]\n",
    "ff = ma.masked_array(np.broadcast_to(v,(lx.shape[0],v.shape[1])),lx)\n",
    "print(ff)\n",
    "ff.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tt)\n",
    "print(lx)\n",
    "tt[lx]\n",
    "bb[lx] = vv * lx\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF',\n",
    "     'TotalBsmtSF','BsmtFullBath','BsmtHalfBath']\n",
    "f2 = ['GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond']\n",
    "#GarageType (Garage location) - has type Basment investigate how it correlates with basment category.\n",
    "f3 = ['GarageYrBlt','MoSold','YrSold','YearBuilt','YearRemodAdd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds = ['GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond','YearBuilt','YearRemodAdd']\n",
    "f = df[flds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.PairGrid(g).map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePrice:\n",
    "    train = 0\n",
    "    test = 1\n",
    "    def __init__(self, train_path, test_path):\n",
    "        train = pd.read_csv(train_path)\n",
    "        train['Id'] = train\n",
    "        test = pd.read_csv(test_path)\n",
    "        test[['SalePrice', 'Id']] = (0.0, test)\n",
    "        self.mlsets = pd.concat([train, test],ignore_index = True) \n",
    "        self.trainlx = self.mlsets['Id'] == train\n",
    "        self.testlx = self.mlsets['Id'] == test\n",
    "        \n",
    "#################### Treat Nan values #############################################################\n",
    "    def __garage_basement_common_nan_handle(self, akeyfld, aflds, avalue):\n",
    "        lx = (self.mlsets[akeyfld] == 0) & (self.mlsets[aflds].isnull().any(axis = 1))\n",
    "        self.mlsets.loc[lx,aflds] = self.mlsets[lx][aflds].fillna(value = avalue)\n",
    "            \n",
    "    def __garage_nan_handle(self):\n",
    "        lx = self.mlsets['GarageYrBlt'].isnull()\n",
    "        self.mlsets.loc[lx,'GarageYrBlt'] = self.mlsets[lx]['YearRemodAdd']\n",
    "        lx = self.mlsets['GarageCars'].isnull()\n",
    "        self.mlsets.loc[lx,'GarageCars'] = self.mlsets[~lx]['GarageCars'].mean()\n",
    "        lx = self.mlsets['GarageArea'].isnull()\n",
    "        self.mlsets.loc[lx,'GarageArea'] = self.mlsets[~lx]['GarageArea'].median()\n",
    "        self.__garage_basement_common_nan_handle('GarageArea', ['GarageType','GarageFinish','GarageQual','GarageCond'], 'None')\n",
    "        self.__garage_basement_common_nan_handle('GarageArea', ['GarageYrBlt','GarageCars'], 0)\n",
    "        \n",
    "    def __basement_nan_handle(self):\n",
    "        df = self.mlsets\n",
    "        numflds = ['BsmtFinSF1', 'BsmtFinSF2']\n",
    "        catflds = ['BsmtFinType1','BsmtFinType2']\n",
    "        for (catfld, numfld) in zip(catflds,numflds):\n",
    "            lx = [numfld].isnull()\n",
    "            tmp = df.groupby([catfld])[numfld].describe()\n",
    "            df.loc[lx,numfld] = np.sum(tmp['50%'] * tmp['count'] / tmp['count'].sum())   \n",
    "        fld = 'TotalBsmtSF'\n",
    "        lx = df[fld].isnull()\n",
    "        df.loc[lx,fld] = df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "        fld = 'BsmtUnfSF'\n",
    "        lx = df[fld].isnull()\n",
    "        df.loc[lx,fld] = df[lx]['TotalBsmtSF'] - df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "        flds = ['BsmtFullBath','BsmtHalfBath'] \n",
    "        lx = df['BsmtFullBath'].isnull()\n",
    "        df.loc[lx,['BsmtFullBath','BsmtHalfBath']]= df[~lx][['BsmtFullBath','BsmtHalfBath']].mean(axis = 0).values  \n",
    "        self.__garage_basement_common_nan_handle('TotalBsmtSF', ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'], 'None')\n",
    "        self.__garage_basement_common_nan_handle('TotalBsmtSF', ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath'], 0)\n",
    "\n",
    "    def __from_nan_to_None(self):\n",
    "        col = ['FireplaceQu','Alley','PoolQC','Fence','MiscFeature']\n",
    "        self.mlsets[col] = test[col].fillna(value = 'None')\n",
    "        \n",
    "    def __MasVnrArea_nan_handle(self):\n",
    "        df = self.mlsets\n",
    "        lx = (df['MasVnrArea'] != 0) & (df['MasVnrType'] == 'None')\n",
    "        df.loc[lx,'MasVnrArea'] = 0.0\n",
    "        lx = (df['MasVnrArea'] == 0) & (df['MasVnrType'] != 'None')\n",
    "        df.loc[lx,'MasVnrType'] = 'None'\n",
    "        lx = df['MasVnrArea'].isnull()\n",
    "        tmp = df.groupby(['MasVnrType'])['MasVnrArea'].describe()\n",
    "        df.loc[lx,'MasVnrArea'] = np.sum(tmp['50%'] * tmp['count'] / tmp['count'].sum()) \n",
    "    \n",
    "    def __find_regresion(self,df,fld):\n",
    "    scaler = QuantileTransformer()\n",
    "    param_grid = {'alpha': [1e0,0.5, 0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6],'gamma': np.logspace(-4, 2, 14)}\n",
    "    solver = GridSearchCV(KernelRidge(kernel = 'rbf'), cv=5, param_grid=param_grid)\n",
    "    #solver = linear_model.RidgeCV(alphas = np.arange(0.001, 5, 0.001))\n",
    "    \n",
    "    #param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6,1e-7]}\n",
    "    #solver = GridSearchCV(Ridge(max_iter = 10000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "    #scaler_x = RobustScaler() #make_pipeline(RobustScaler(),PowerTransformer())\n",
    "    #scaler_y = PowerTransformer()\n",
    "    scaler_x = QuantileTransformer() \n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    lx = df[fld].isnull()\n",
    "    dfy = df[fld]\n",
    "    dfx = df.drop(fld, axis = 1)\n",
    "    X = dfx[~lx].as_matrix().astype('float')\n",
    "    y = dfy[~lx].ravel()\n",
    "    #scaler_y = scaler_y.fit(y.reshape(-1,1))\n",
    "    #y = scaler_y.transform(y.reshape(-1,1)).ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "    scaler_x = scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "\n",
    "    solver.fit(X_train, y_train)\n",
    "    print(solver.score(X_test, y_test))\n",
    "    X = scaler_x.fit_transform(X)\n",
    "    XX = scaler_x.transform(dfx[lx].as_matrix().astype('float'))\n",
    "    solver.fit(X, y) \n",
    "    yy = solver.predict(XX).astype('float')\n",
    "    dfy[lx] = yy#scaler_y.inverse_transform(yy.reshape(-1,1)).ravel()\n",
    "    return dfy\n",
    "\n",
    "  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainTestSets(train_path, test_path):   \n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    col = ['FireplaceQu','Alley','PoolQC','Fence','MiscFeature']\n",
    "    train[col] = train[col].fillna(value = 'None')\n",
    "    test[col] = test[col].fillna(value = 'None')\n",
    "    \n",
    "    catcol = [['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'],\n",
    "              ['GarageType','GarageFinish','GarageQual','GarageCond']]\n",
    "    numcol = [['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath'],\n",
    "              ['GarageYrBlt','GarageCars','GarageArea']]\n",
    "    keyflds = ['TotalBsmtSF','GarageArea']\n",
    "    for (catflds,numflds,keyfld) in zip(catcol, numcol, keyflds):\n",
    "        \n",
    "        lx = (train[keyfld] == 0) & (train[catflds].isnull().any(axis = 1))\n",
    "        train.loc[lx,catflds] = train[lx][catflds].fillna(value = 'None')\n",
    "        \n",
    "        lx = (train[keyfld] == 0) & (train[numflds].isnull().any(axis = 1))\n",
    "        train.loc[lx,numflds] = train[lx][numflds].fillna(value = 0)\n",
    "        \n",
    "        lx = (test[keyfld] == 0) & (test[catflds].isnull().any(axis = 1))\n",
    "        test.loc[lx,catflds] = test[lx][catflds].fillna(value = 'None')\n",
    "        \n",
    "        lx = (test[keyfld] == 0) & (test[numflds].isnull().any(axis = 1))\n",
    "        test.loc[lx,numflds] = test[lx][numflds].fillna(value = 0)\n",
    "        \n",
    "    train = train.drop('Utilities', axis = 1)\n",
    "    test = test.drop('Utilities', axis = 1)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "g1 = pd.DataFrame({'a' : [1,2,3], 'b' : ['11', '22', '33']})\n",
    "class Tr(BaseEstimator):\n",
    "    def fit(self,X,y = None):\n",
    "        self.columns = X.columns\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X[X.columns[0]] += 4\n",
    "        print(X.head())\n",
    "        return X['a']\n",
    "    def fit_transform(self,X,y = None):\n",
    "        self = self.fit(X)\n",
    "        return self.transform(X)\n",
    "    def get_feature_names(self):\n",
    "        return self.columns\n",
    "    \n",
    "column_trans = ColumnTransformer([('a_category', Tr(), ['a','b']),#Pipeline([('s1' ,Tr()), ('s2',Tr())]), ['a']),\n",
    "                                  ('b_category', OneHotEncoder(dtype = 'float'), ['b','a'])],\n",
    "                                 remainder='drop')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1[['a']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = column_trans.fit_transform(g1)\n",
    "#p = Pipeline([('b', Tr()),('a', column_trans)])\n",
    "#rr=p.fit_transform(g1)\n",
    "print(rr)\n",
    "print(column_trans.get_feature_names())\n",
    "#g2 = pd.DataFrame(data = rr, columns = column_trans.get_feature_names)\n",
    "#gg=Tr()\n",
    "#g1[g1.columns[0]] += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = GetTrainTestSets(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessOutliers(train, test):\n",
    "    lx1 = (train['SalePrice'] <= 200000) & (train['GrLivArea'] >= 4000) \n",
    "    lx2 = train['LotFrontage'] > 300 \n",
    "    lx = lx1 | lx2\n",
    "    train = train[~lx]\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessFeature(train, test):\n",
    "    #train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRegresion(df,fld):\n",
    "    scaler = QuantileTransformer()\n",
    "    param_grid = {'alpha': [1e0,0.5, 0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6],'gamma': np.logspace(-4, 2, 14)}\n",
    "    solver = GridSearchCV(KernelRidge(kernel = 'rbf'), cv=5, param_grid=param_grid)\n",
    "    #solver = linear_model.RidgeCV(alphas = np.arange(0.001, 5, 0.001))\n",
    "    \n",
    "    #param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6,1e-7]}\n",
    "    #solver = GridSearchCV(Ridge(max_iter = 10000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "    #scaler_x = RobustScaler() #make_pipeline(RobustScaler(),PowerTransformer())\n",
    "    #scaler_y = PowerTransformer()\n",
    "    scaler_x = QuantileTransformer() \n",
    "    \n",
    "    df = pd.get_dummies(df)\n",
    "    lx = df[fld].isnull()\n",
    "    dfy = df[fld]\n",
    "    dfx = df.drop(fld, axis = 1)\n",
    "    X = dfx[~lx].as_matrix().astype('float')\n",
    "    y = dfy[~lx].ravel()\n",
    "    #scaler_y = scaler_y.fit(y.reshape(-1,1))\n",
    "    #y = scaler_y.transform(y.reshape(-1,1)).ravel()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "    scaler_x = scaler_x.fit(X_train)\n",
    "    X_train = scaler_x.transform(X_train)\n",
    "    X_test = scaler_x.transform(X_test)\n",
    "\n",
    "    solver.fit(X_train, y_train)\n",
    "    print(solver.score(X_test, y_test))\n",
    "    X = scaler_x.fit_transform(X)\n",
    "    XX = scaler_x.transform(dfx[lx].as_matrix().astype('float'))\n",
    "    solver.fit(X, y) \n",
    "    yy = solver.predict(XX).astype('float')\n",
    "    dfy[lx] = yy#scaler_y.inverse_transform(yy.reshape(-1,1)).ravel()\n",
    "    return dfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessAll(df):\n",
    "    df['MSSubClass'] = df['MSSubClass'].apply(str)\n",
    "    df['OverallCond'] = df['OverallCond'].astype(str)\n",
    "    df['YrSold'] = df['YrSold'].astype(str)\n",
    "    df['MoSold'] = df['MoSold'].astype(str)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessNumNans(df):\n",
    "    fld = 'LotFrontage'\n",
    "    df.loc[:,[fld]] = FindRegresion(df[['LotFrontage','Neighborhood','LotConfig','LotArea']],fld)\n",
    "    \n",
    "    lx = (df['MasVnrArea'] != 0) & (df['MasVnrType'] == 'None')\n",
    "    df.loc[lx,'MasVnrArea'] = 0.0\n",
    "    lx = (df['MasVnrArea'] == 0) & (df['MasVnrType'] != 'None')\n",
    "    df.loc[lx,'MasVnrType'] = 'None'\n",
    "    \n",
    "    numflds = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2']\n",
    "    catflds = ['MasVnrType', 'BsmtFinType1','BsmtFinType2']\n",
    "\n",
    "    for (catfld, numfld) in zip(catflds,numflds):\n",
    "        lx = df[numfld].isnull()\n",
    "        tmp = df.groupby([catfld])[numfld].describe()\n",
    "        df.loc[lx,numfld] = np.sum(tmp['50%'] * tmp['count'] / tmp['count'].sum())   \n",
    "        \n",
    "    fld = 'TotalBsmtSF'\n",
    "    lx = df[fld].isnull()\n",
    "    df.loc[lx,fld] = df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "    \n",
    "    fld = 'BsmtUnfSF'\n",
    "    lx = df[fld].isnull()\n",
    "    df.loc[lx,fld] = df[lx]['TotalBsmtSF'] - df[lx][['BsmtFinSF1', 'BsmtFinSF2']].sum(axis = 1)\n",
    "    \n",
    "    flds = ['BsmtFullBath','BsmtHalfBath'] \n",
    "    lx = df['BsmtFullBath'].isnull()\n",
    "    df.loc[lx,['BsmtFullBath','BsmtHalfBath']]= df[~lx][['BsmtFullBath','BsmtHalfBath']].mean(axis = 0).values\n",
    "    \n",
    "    lx = df['GarageYrBlt'].isnull()\n",
    "    df.loc[lx,'GarageYrBlt'] = df[lx]['YearRemodAdd']\n",
    "    lx = df['GarageCars'].isnull()\n",
    "    df.loc[lx,'GarageCars'] = df[~lx]['GarageCars'].mean()\n",
    "    lx = df['GarageArea'].isnull()\n",
    "    df.loc[lx,'GarageArea'] = df[~lx]['GarageArea'].median()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProccessCategoricalNans(df):\n",
    "    oldflds = []\n",
    "    newdf = pd.DataFrame()\n",
    "######################## 1) MSZoning - 'RL', 'RM', 'C (all)', 'FV', 'RH', nan ############################\n",
    "    colname = 'MSZoning'\n",
    "    oldflds.append(colname)\n",
    "    #### with Mitchel neighbor it is highly likely that zoning is 'RL' (grouping by Neighbors + BuiltYear)\n",
    "    lx=(df[colname].isnull()) & (df['Neighborhood'] == 'Mitchel')  \n",
    "    df.loc[lx,colname] = 'RL'\n",
    "    ### group by IDOTRR neighbor\n",
    "    invalidlx = df[colname].isnull() \n",
    "    idotrrlx = df['Neighborhood'] == 'IDOTRR'\n",
    "\n",
    "    tempdf = pd.get_dummies(df[colname],prefix = colname).astype(float)\n",
    "    tempdf.loc[invalidlx,:] = tempdf[idotrrlx].sum().values / tempdf[idotrrlx].sum().sum()\n",
    "    newdf[tempdf.columns] = tempdf\n",
    "######################## 2) ordered categorias\n",
    "    catflds = ['FireplaceQu','ExterQual','ExterCond','HeatingQC','PoolQC','LandSlope','LotShape',\n",
    "               'PavedDrive','Street','Alley','CentralAir','MSSubClass','OverallCond','YrSold','MoSold']\n",
    "    df['FireplaceQu'] = pd.Categorical(df['FireplaceQu'].values,  ['None','Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['ExterQual'] = pd.Categorical(df['ExterQual'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['ExterCond'] = pd.Categorical(df['ExterCond'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['HeatingQC'] = pd.Categorical(df['HeatingQC'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['PoolQC'] = pd.Categorical(df['PoolQC'].values, ['None', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['LandSlope'] = pd.Categorical(df['LandSlope'].values, ['Sev', 'Mod', 'Gtl'], ordered = True)\n",
    "    df['LotShape'] = pd.Categorical(df['LotShape'].values, ['Reg', 'IR1', 'IR2', 'IR3'], ordered = True)\n",
    "    df['PavedDrive'] = pd.Categorical(df['PavedDrive'].values, ['N', 'P', 'Y'], ordered = True)\n",
    "    df['Street'] = pd.Categorical(df['Street'].values, ['Grvl','Pave'], ordered = True)\n",
    "    df['Alley'] = pd.Categorical(df['Alley'].values, ['None', 'Grvl', 'Pave'], ordered = True)\n",
    "    df['CentralAir'] = pd.Categorical(df['CentralAir'].values, ['N', 'Y'], ordered = True)\n",
    "    df['MSSubClass'] = pd.Categorical(df['MSSubClass'].values, ['20','30','40','45','50','60','70','75','80','85','90','120','150','160','180','190'], ordered = True)\n",
    "    df['OverallCond'] = pd.Categorical(df['OverallCond'].values, ['1','2','3','4','5','6','7','8','9'], ordered = True)\n",
    "    df['YrSold'] = pd.Categorical(df['YrSold'].values, ['2006','2007','2008','2009','2010'], ordered = True)\n",
    "    df['MoSold'] = pd.Categorical(df['MoSold'].values, ['1','2','3','4','5','6','7','8','9','10','11','12'], ordered = True)\n",
    "\n",
    "    nanflds = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','KitchenQual',\n",
    "               'GarageFinish', 'GarageQual','GarageCond']\n",
    "    df['BsmtQual'] = pd.Categorical(df['BsmtQual'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['BsmtCond'] = pd.Categorical(df['BsmtCond'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['BsmtExposure'] = pd.Categorical(df['BsmtExposure'].values, ['None', 'No', 'Mn', 'Av', 'Gd'], ordered = True)\n",
    "    df['BsmtFinType1'] = pd.Categorical(df['BsmtFinType1'].values, ['None','Unf','LwQ', 'Rec','BLQ','ALQ','GLQ'], ordered = True)\n",
    "    df['BsmtFinType2'] = pd.Categorical(df['BsmtFinType2'].values, ['None','Unf','LwQ', 'Rec','BLQ','ALQ','GLQ'], ordered = True)\n",
    "    df['KitchenQual'] = pd.Categorical(df['KitchenQual'].values, ['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['GarageFinish'] = pd.Categorical(df['GarageFinish'].values, ['None', 'Unf', 'RFn', 'Fin'], ordered = True)\n",
    "    df['GarageQual'] = pd.Categorical(df['GarageQual'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df['GarageCond'] = pd.Categorical(df['GarageCond'].values, ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered = True)\n",
    "    df.loc[:,catflds + nanflds] = df[catflds + nanflds].apply(lambda x: x.cat.codes).astype('float')\n",
    "    for fld in nanflds:\n",
    "        lx = df[fld] < 0.0\n",
    "        df.loc[lx,fld] = df[~lx][fld].mean()\n",
    "######################## 3) 'Exterior1st', 'Exterior2nd','MasVnrType','Electrical','KitchenQual'#############\n",
    "########################'BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'##################\n",
    "######################## 'Functional','GarageType', 'GarageFinish', 'GarageQual','GarageCond'################\n",
    "######################## 'PoolQC','SaleType'#################################################################   \n",
    "    catflds = ['Exterior1st','Exterior2nd','MasVnrType','Electrical','GarageType','SaleType','Functional']\n",
    "    for fld in catflds:\n",
    "        lx = df[fld].isnull()\n",
    "        oldflds.append(fld)\n",
    "        tmp = pd.get_dummies(df[fld],prefix = fld).astype(float)\n",
    "        tmp.loc[lx,:] = tmp[~lx].sum().values / tmp[~lx].sum().sum()\n",
    "        newdf[tmp.columns] = tmp\n",
    "    df = df.drop(labels = oldflds,axis = 1)\n",
    "    df[newdf.columns] = newdf\n",
    "    df = pd.get_dummies(df).astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FormatAndSave(df):\n",
    "    processed_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "    write_train_path = os.path.join(processed_data_path, 'train.csv')\n",
    "    write_test_path = os.path.join(processed_data_path, 'test.csv')\n",
    "    train_df = df[df['SalePrice'] != 0]\n",
    "    train_df.loc[:,'Id'] = train_df['Id'].astype('int32').values\n",
    "    test_df = df[df['SalePrice'] == 0]\n",
    "    test_df = test_df.drop(labels = 'SalePrice',axis = 1)\n",
    "    test_df.loc[:,'Id'] = test_df['Id'].astype('int32').values\n",
    "    train_df.to_csv(write_train_path)\n",
    "    test_df.to_csv(write_test_path) \n",
    "    return (df,train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetResultData(train_path, test_path):\n",
    "    (train, test) = GetTrainTestSets(train_path, test_path)\n",
    "    (train, test) = ProccessOutliers(train, test)\n",
    "    (train, test) = ProcessFeature(train, test)\n",
    "    train1 = train\n",
    "    test1 = test\n",
    "    test['SalePrice'] = 0\n",
    "    return (train.append(test), train1, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df, train, test) = GetResultData(train_path, test_path)\n",
    "#df = ProccessNumNans(df)\n",
    "(df, train, test) = df.pipe(PreprocessAll).pipe(ProccessNumNans).pipe(ProccessCategoricalNans).pipe(FormatAndSave)\n",
    "#best score = 0.627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl.matplotlib_fname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['SalePrice'].values\n",
    "sns.JointGrid(x = 'SalePrice', y = 'GrLivArea', data=train[['SalePrice','GrLivArea']]).plot(sns.scatterplot, sns.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['SalePrice','GrLivArea']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds =['SalePrice','GrLivArea', 'GarageArea', 'LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\n",
    "#sns.PairGrid(train[flds]).map(plt.scatter)\n",
    "cor = train.corr()\n",
    "#sns.distplot(cor['SalePrice'])\n",
    "cor['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = os.path.join(os.path.pardir,'data','processed')\n",
    "train_path = os.path.join(processed_data_path, 'train.csv')\n",
    "test_path = os.path.join(processed_data_path, 'test.csv')\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df = test_df.drop(labels = ['Unnamed: 0'],axis = 1)\n",
    "train_df = train_df.drop(labels = ['Id','Unnamed: 0'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_ml_solve(train_df, test_df, scaler, solver, q = 0.4, target = 'temp.csv'):\n",
    "    collx = train_df.columns != 'SalePrice'\n",
    "    X = train_df.loc[:,collx ].as_matrix().astype('float')\n",
    "    y = train_df['SalePrice'].ravel()\n",
    "    scaler_y = RobustScaler()#PowerTransformer()\n",
    "    scaler_y = scaler_y.fit(y.reshape(-1,1))\n",
    "    #y = scaler_y.transform(y.reshape(-1,1))\n",
    "    y = np.log1p(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=q, random_state=0)\n",
    "    #scaler = scaler.fit(X_train)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    sscore = 0.0\n",
    "    if q != 0:\n",
    "        X_test = scaler.transform(X_test)\n",
    "        solver.fit(X_train, y_train)\n",
    "        sscore = solver.score(X_test, y_test)\n",
    "    else: \n",
    "        print(\"No test subsets!!!!\")\n",
    "    scaler = scaler.fit(X) \n",
    "    X = scaler.transform(X)  \n",
    "    ids = test_df['Id'].values\n",
    "    test_df = test_df.drop(labels = 'Id',axis = 1)\n",
    "    XX = scaler.transform(test_df.as_matrix().astype('float'))\n",
    "    solver.fit(X, y) \n",
    "    pred_y = solver.predict(XX).astype('float')\n",
    "    df_submission = pd.DataFrame({'Id': ids, 'SalePrice' : np.exp(pred_y) - 1.0} )\n",
    "    #df_submission = pd.DataFrame({'Id': ids, 'SalePrice' : scaler_y.inverse_transform(pred_y.reshape(-1,1)).ravel()} )\n",
    "    submission_data_path = os.path.join(os.path.pardir,'data','external')\n",
    "    submission_file_path = os.path.join(submission_data_path, target)\n",
    "    df_submission.to_csv(submission_file_path, index=False)\n",
    "    print(sscore)\n",
    "    return (df_submission, solver, sscore) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = RobustScaler()\n",
    "scaler4 = Normalizer()\n",
    "scaler5 = QuantileTransformer()\n",
    "scaler6 = PowerTransformer()\n",
    "scaler7 = make_pipeline(RobustScaler(), PowerTransformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge Linear Regresion\n",
    "param_grid = {'alpha': [7.0,6.0,5.0,3.0,2.0,1.5,1.3,1.2,1.1,1.001,0.8,0.5,0.2,0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6,1e-7]}\n",
    "solver = GridSearchCV(Ridge(max_iter = 10000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "scaler = RobustScaler()\n",
    "(ridgedm, ridgesolver, ridgescore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'Ridged.csv')\n",
    "print(ridgesolver.best_params_)\n",
    "print(ridgesolver.best_score_)\n",
    "#max score = 0.9158041549014316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso Linear Regresion\n",
    "#param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7]}\n",
    "param_grid = {'alpha': [ 0.04,0.03, 0.02,0.001, 0.000888889, 0.0008]}\n",
    "solver = GridSearchCV(Lasso(max_iter = 25000,tol = 0.000001), cv=5,param_grid=param_grid)\n",
    "solver = Lasso(max_iter = 25000,tol = 0.000001, alpha = 0.000888889)\n",
    "scaler = RobustScaler()\n",
    "(lassom, lassomsolver, lassomscore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'Lasso.csv')\n",
    "#print(lassomsolver.best_params_)\n",
    "#print(lassomsolver.best_score_)\n",
    "#max score = 0.9183107282670468\n",
    "#0.00089 0.9183148121017135\n",
    "#0.000889 0.9183151760580683\n",
    "#alpha = 0.000888889 score = 0.9183152160821689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoLARS Linear Regresion\n",
    "param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.8,0.5,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7]}\n",
    "param_grid = {'alpha': [0.001, 0.00088888]}\n",
    "solver = GridSearchCV(Lasso(max_iter = 25000,tol = 0.000001), cv=5,param_grid=param_grid)\n",
    "#solver = LassoLars(max_iter = 25000,alpha = 0.00088888)\n",
    "scaler = RobustScaler()\n",
    "(lassolarsm, lassolarsmsolver, lassolarsmscore) = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'LassoLars.csv')\n",
    "print(lassolarsmsolver.best_params_)\n",
    "print(lassolarsmsolver.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LassoLarsIC Linear Regresion\n",
    "solver = LassoLarsIC(max_iter = 25000, criterion = 'aic')\n",
    "scaler = RobustScaler()\n",
    "lassolarsICm = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'LassoLarsIC.csv')\n",
    "#max score = 0.8964009621989726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in np.logspace(start = 0.0001, stop = 0.99, num=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet Regresion\n",
    "param_grid = {'alpha': [5.0,3.0,2.0,1.5,1.3,1.2,1.1,1e0,0.99,0.9,0.8,0.6,0.5,0.4,0.3,0.25,0.2,0.1, 1e-2,0.05,1e-3,0.005, 1e-4,0.0005, 1e-5,0.000051e-6,1e-7],\n",
    "             'l1_ratio' : [1e-3,5e-3,1e-2,5e-2,0.1,0.2,0.25,0.3,0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.901,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99,0.995,0.999]}\n",
    "solver = GridSearchCV(ElasticNet(max_iter = 25000,tol = 0.00001), cv=5,param_grid=param_grid)\n",
    "print(solver.estimator.alpha)\n",
    "scaler = RobustScaler()\n",
    "elastic_net_m = linear_ml_solve(train_df, test_df, scaler, solver, 0.4,'ElasticNet.csv')\n",
    "#max_score = 0.9183288770325282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver1 = linear_model.RidgeCV(alphas = np.arange(0.001, 5, 0.001))\n",
    "solver2 = linear_model.LassoCV()\n",
    "solver3 = linear_model.ElasticNetCV(alphas = np.arange(0.001, 5, 0.001), max_iter = 20000 )\n",
    "solver4 = linear_model.BayesianRidge()\n",
    "solver6 = linear_model.PassiveAggressiveRegressor(C = 10,tol = 0.00001, max_iter = 50000)\n",
    "solver7 = linear_model.HuberRegressor(max_iter=10000)\n",
    "solver8 = Pipeline([('poly', PolynomialFeatures(degree=2)),  ('linear', solver1)])\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [1e0,0.5, 0.1, 1e-2, 1e-3,1e-4,1e-5,1e-6],\n",
    "    'gamma': np.logspace(-4, 2, 14)\n",
    "}\n",
    "solver5_1 = GridSearchCV(KernelRidge(kernel = 'polynomial',degree = 80), cv=8, param_grid=param_grid)\n",
    "solver5_2 = GridSearchCV(KernelRidge(kernel = 'rbf'), cv=8, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subm = linear_ml_solve(train_df, test_df, scaler5, solver2, 0.4,'11_LassoCV.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
