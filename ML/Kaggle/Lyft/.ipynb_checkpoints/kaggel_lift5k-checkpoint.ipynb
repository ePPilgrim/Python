{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "#-----------------l5kit lib ------------------------------------------\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import (\n",
    "    LocalDataManager, ChunkedDataset, \n",
    "    get_agents_slice_from_frames, get_frames_slice_from_scenes,get_tl_faces_slice_from_frames,\n",
    "    filter_agents_by_labels, filter_tl_faces_by_frames\n",
    ")\n",
    "from l5kit.data.labels import PERCEPTION_LABEL_TO_INDEX \n",
    "from l5kit.data.filter import filter_agents_by_labels, filter_agents_by_track_id, filter_agents_by_frames\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import (\n",
    "    build_rasterizer, Rasterizer, RenderContext,\n",
    "    EGO_EXTENT_HEIGHT, EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH\n",
    ")\n",
    "from l5kit.rasterization.semantic_rasterizer import CV2_SHIFT, cv2_subpixel\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points, angular_distance, compute_agent_pose, rotation33_as_yaw,transform_point\n",
    "from l5kit.geometry.transform import yaw_as_rotation33\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.kinematic import Perturbation\n",
    "from l5kit.sampling.slicing import get_future_slice, get_history_slice\n",
    "from l5kit.data.zarr_dataset import AGENT_DTYPE\n",
    "\n",
    "#------------ Tensorflow lib ---------------------------------------------\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': \"EfficientNetB0\"\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5,\n",
    "        'disable_traffic_light_faces': False\n",
    "    },\n",
    "    \n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "MIN_FRAME_HISTORY = 10\n",
    "MIN_FRAME_FUTURE = 1\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "dm = LocalDataManager(None)\n",
    "raster_w = cfg[\"raster_params\"][\"raster_size\"][0]\n",
    "raster_h = cfg[\"raster_params\"][\"raster_size\"][1]\n",
    "input_deep = 8\n",
    "length_of_rnn_sequence = cfg[\"model_params\"][\"history_num_frames\"] + 1\n",
    "future_output_shape = (cfg[\"model_params\"][\"future_num_frames\"], 2)\n",
    "BATCHSIZE = cfg[\"train_data_loader\"][\"batch_size\"]\n",
    "inner_cnn_model = cfg[\"model_params\"][\"model_architecture\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterExpander():\n",
    "    def __init__(self, render_context: RenderContext, filter_agents_threshold: float, history_num_frames: int):\n",
    "        self.render_context = render_context\n",
    "        self.raster_size = render_context.raster_size_px\n",
    "        self.filter_agents_threshold = filter_agents_threshold\n",
    "        self.history_num_frames = history_num_frames\n",
    "        self.labels = ['PERCEPTION_LABEL_CAR', 'PERCEPTION_LABEL_BICYCLE', 'PERCEPTION_LABEL_PEDESTRIAN']\n",
    "        self.num_of_layers = len(self.labels) + 2    \n",
    "\n",
    "    def AddDeepness(self, history_frames: np.ndarray, history_agents: List[np.ndarray], \n",
    "                    history_tl_faces: List[np.ndarray], agent: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        frame = history_frames[0]\n",
    "        if agent is None:\n",
    "            ego_translation_m = history_frames[0][\"ego_translation\"]\n",
    "            ego_yaw_rad = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        else:\n",
    "            ego_translation_m = np.append(agent[\"centroid\"], history_frames[0][\"ego_translation\"][-1])\n",
    "            ego_yaw_rad = agent[\"yaw\"]\n",
    "\n",
    "        raster_from_world = self.render_context.raster_from_world(ego_translation_m, ego_yaw_rad)\n",
    "        out_shape = (self.raster_size[1], self.raster_size[0], self.history_num_frames + 1, self.num_of_layers)\n",
    "        agents_images = np.zeros(out_shape, dtype=np.float32)\n",
    "\n",
    "        for i, (frame, agents) in enumerate(zip(history_frames, history_agents)):\n",
    "            agents = filter_agents_by_labels(agents, self.filter_agents_threshold)\n",
    "            # note the cast is for legacy support of dataset before April 2020\n",
    "            av_agent = self.get_ego_as_agent(frame).astype(agents.dtype)\n",
    "\n",
    "            if agent is None:\n",
    "                agents_image = self.expand_into_deep(self.raster_size, raster_from_world, agents)\n",
    "            else:\n",
    "                agent_ego = filter_agents_by_track_id(agents, agent[\"track_id\"])\n",
    "                if len(agent_ego) != 0:  \n",
    "                    agents = agents[agents != agent_ego[0]]\n",
    "                agents_image = self.expand_into_deep(self.raster_size, raster_from_world, np.append(agents, av_agent))\n",
    "            agents_images[..., i,:self.num_of_layers] = agents_image.transpose((1,2,0))\n",
    "        return agents_images\n",
    "    \n",
    "    def expand_into_deep(self, raster_size: Tuple[int, int], raster_from_world: np.ndarray, agents: np.ndarray) -> np.ndarray:\n",
    "        layers = np.zeros((self.num_of_layers,raster_size[1], raster_size[0]), dtype=np.float32)\n",
    "        box_world_coords = np.zeros((len(agents), 4, 2))\n",
    "        corners_base_coords = np.asarray([[-1, -1], [-1, 1], [1, 1], [1, -1]])\n",
    "\n",
    "        for idx, agent in enumerate(agents):\n",
    "            corners = corners_base_coords * agent[\"extent\"][:2] / 2  # corners in zero\n",
    "            r_m = yaw_as_rotation33(agent[\"yaw\"])\n",
    "            box_world_coords[idx] = transform_points(corners, r_m) + agent[\"centroid\"][:2]\n",
    "    \n",
    "        box_raster_coords = transform_points(box_world_coords.reshape((-1, 2)), raster_from_world)\n",
    "        box_raster_coords = cv2_subpixel(box_raster_coords.reshape((-1, 4, 2)))\n",
    "    \n",
    "        for idx, agent in enumerate(agents):\n",
    "            #print(agent[\"label_probabilities\"])\n",
    "            for i, label in enumerate(self.labels):\n",
    "                lix = PERCEPTION_LABEL_TO_INDEX[label]\n",
    "                color = float(agent[\"label_probabilities\"][lix])\n",
    "                cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=color, shift=CV2_SHIFT)\n",
    "            i = len(self.labels)\n",
    "            cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][0]), shift=CV2_SHIFT)\n",
    "            cv2.fillPoly(layers[i + 1,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][1]),shift=CV2_SHIFT)     \n",
    "        return layers\n",
    "    \n",
    "    def get_ego_as_agent(self,frame: np.ndarray) -> np.ndarray:  # TODO this can be useful to have around\n",
    "        ego_agent = np.zeros(1, dtype=AGENT_DTYPE)\n",
    "        ego_agent[0][\"centroid\"] = frame[\"ego_translation\"][:2]\n",
    "        ego_agent[0][\"yaw\"] = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        ego_agent[0][\"extent\"] = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n",
    "        return ego_agent\n",
    "    \n",
    "def _create_targets_for_deep_prediction(num_frames: int, frames: np.ndarray, selected_track_id: Optional[int], \n",
    "                                        agents: List[np.ndarray], agent_from_world: np.ndarray,current_agent_yaw: float\n",
    "                                       ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    coords_offset = np.zeros((num_frames, 2), dtype=np.float32)\n",
    "    yaws_offset = np.zeros((num_frames, 1), dtype=np.float32)\n",
    "    availability = np.zeros((num_frames,), dtype=np.float32)\n",
    "\n",
    "    for i, (frame, frame_agents) in enumerate(zip(frames, agents)):\n",
    "        if selected_track_id is None:\n",
    "            agent_centroid = frame[\"ego_translation\"][:2]\n",
    "            agent_yaw = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        else:\n",
    "            # it's not guaranteed the target will be in every frame\n",
    "            try:\n",
    "                agent = filter_agents_by_track_id(frame_agents, selected_track_id)[0]\n",
    "                agent_centroid = agent[\"centroid\"]\n",
    "                agent_yaw = agent[\"yaw\"]\n",
    "            except IndexError:\n",
    "                availability[i] = 0.0  # keep track of invalid futures/history\n",
    "                continue\n",
    "\n",
    "        coords_offset[i] = transform_point(agent_centroid, agent_from_world)\n",
    "        yaws_offset[i] = angular_distance(agent_yaw, current_agent_yaw)\n",
    "        availability[i] = 1.0\n",
    "    return coords_offset, yaws_offset, availability\n",
    "\n",
    "def decorated_generate_agent_sample(state_index: int, frames: np.ndarray, agents: np.ndarray, tl_faces: np.ndarray,\n",
    "                                    selected_track_id: Optional[int], render_context: RenderContext, history_num_frames: int,\n",
    "                                    history_step_size: int, future_num_frames: int, future_step_size: int,\n",
    "                                    filter_agents_threshold: float, rasterizer: Rasterizer) -> dict:\n",
    "    #  the history slice is ordered starting from the latest frame and goes backward in time., ex. slice(100, 91, -2)\n",
    "    history_slice = get_history_slice(state_index, history_num_frames, history_step_size, include_current_state=True)\n",
    "    future_slice = get_future_slice(state_index, future_num_frames, future_step_size)\n",
    "    history_frames = frames[history_slice].copy()  # copy() required if the object is a np.ndarray\n",
    "    future_frames = frames[future_slice].copy()\n",
    "    sorted_frames = np.concatenate((history_frames[::-1], future_frames))  # from past to future\n",
    "    agent_slice = get_agents_slice_from_frames(sorted_frames[0], sorted_frames[-1])\n",
    "    agents = agents[agent_slice].copy()  # this is the minimum slice of agents we need\n",
    "    history_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n",
    "    future_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n",
    "    history_agents = filter_agents_by_frames(history_frames, agents)\n",
    "    future_agents = filter_agents_by_frames(future_frames, agents)\n",
    "    tl_slice = get_tl_faces_slice_from_frames(history_frames[-1], history_frames[0])  # -1 is the farthest\n",
    "    # sync interval with the traffic light faces array\n",
    "    history_frames[\"traffic_light_faces_index_interval\"] -= tl_slice.start\n",
    "    history_tl_faces = filter_tl_faces_by_frames(history_frames, tl_faces[tl_slice].copy())\n",
    "    # State you want to predict the future of.\n",
    "    cur_frame = history_frames[0]\n",
    "    cur_agents = history_agents[0]\n",
    "\n",
    "    if selected_track_id is None:\n",
    "        agent_centroid_m = cur_frame[\"ego_translation\"][:2]\n",
    "        agent_yaw_rad = rotation33_as_yaw(cur_frame[\"ego_rotation\"])\n",
    "        agent_extent_m = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n",
    "        selected_agent = None\n",
    "    else:\n",
    "        try:\n",
    "            agent = filter_agents_by_track_id(\n",
    "                filter_agents_by_labels(cur_agents, filter_agents_threshold), selected_track_id\n",
    "            )[0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\" track_id {selected_track_id} not in frame or below threshold\")\n",
    "        agent_centroid_m = agent[\"centroid\"]\n",
    "        agent_yaw_rad = float(agent[\"yaw\"])\n",
    "        agent_extent_m = agent[\"extent\"]\n",
    "        selected_agent = agent\n",
    "\n",
    "    input_im = (\n",
    "        None\n",
    "        if not rasterizer\n",
    "        else rasterizer.rasterize(history_frames, history_agents, history_tl_faces, selected_agent)\n",
    "    )\n",
    "    \n",
    "    rasterExpander = RasterExpander(rasterizer.render_context, rasterizer.filter_agents_threshold,rasterizer.history_num_frames)\n",
    "    deepLayers = rasterExpander.AddDeepness(history_frames, history_agents, history_tl_faces, selected_agent)\n",
    "\n",
    "    world_from_agent = compute_agent_pose(agent_centroid_m, agent_yaw_rad)\n",
    "    agent_from_world = np.linalg.inv(world_from_agent)\n",
    "    raster_from_world = render_context.raster_from_world(agent_centroid_m, agent_yaw_rad)\n",
    "\n",
    "    future_coords_offset, future_yaws_offset, future_availability = _create_targets_for_deep_prediction(\n",
    "        future_num_frames, future_frames, selected_track_id, future_agents, agent_from_world, agent_yaw_rad\n",
    "    )\n",
    "\n",
    "    # history_num_frames + 1 because it also includes the current frame\n",
    "    history_coords_offset, history_yaws_offset, history_availability = _create_targets_for_deep_prediction(\n",
    "        history_num_frames + 1, history_frames, selected_track_id, history_agents, agent_from_world, agent_yaw_rad\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"image\": input_im,\n",
    "        \"deep_layers\": deepLayers,\n",
    "        \"target_positions\": future_coords_offset,\n",
    "        \"target_yaws\": future_yaws_offset,\n",
    "        \"target_availabilities\": future_availability,\n",
    "        \"history_positions\": history_coords_offset,\n",
    "        \"history_yaws\": history_yaws_offset,\n",
    "        \"history_availabilities\": history_availability,\n",
    "        \"world_to_image\": raster_from_world,  # TODO deprecate\n",
    "        \"raster_from_agent\": raster_from_world @ world_from_agent,\n",
    "        \"raster_from_world\": raster_from_world,\n",
    "        \"agent_from_world\": agent_from_world,\n",
    "        \"world_from_agent\": world_from_agent,\n",
    "        \"centroid\": agent_centroid_m,\n",
    "        \"yaw\": agent_yaw_rad,\n",
    "        \"extent\": agent_extent_m,\n",
    "    }\n",
    "\n",
    "class DecoratedAgentDataset(AgentDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: dict,\n",
    "        zarr_dataset: ChunkedDataset,\n",
    "        rasterizer: Rasterizer,\n",
    "        perturbation: Optional[Perturbation] = None,\n",
    "        agents_mask: Optional[np.ndarray] = None,\n",
    "        min_frame_history: int = MIN_FRAME_HISTORY,\n",
    "        min_frame_future: int = MIN_FRAME_FUTURE,\n",
    "    ):\n",
    "        assert perturbation is None, \"AgentDataset does not support perturbation (yet)\"\n",
    "        super(DecoratedAgentDataset, self).__init__(cfg, zarr_dataset, rasterizer, perturbation, agents_mask,\n",
    "                                                    min_frame_history, min_frame_future)\n",
    "        render_context = RenderContext(\n",
    "            raster_size_px=np.array(cfg[\"raster_params\"][\"raster_size\"]),\n",
    "            pixel_size_m=np.array(cfg[\"raster_params\"][\"pixel_size\"]),\n",
    "            center_in_raster_ratio=np.array(cfg[\"raster_params\"][\"ego_center\"]),\n",
    "        )\n",
    "        self.sample_function = partial(\n",
    "            decorated_generate_agent_sample,\n",
    "            render_context=render_context,\n",
    "            history_num_frames=cfg[\"model_params\"][\"history_num_frames\"],\n",
    "            history_step_size=cfg[\"model_params\"][\"history_step_size\"],\n",
    "            future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "            future_step_size=cfg[\"model_params\"][\"future_step_size\"],\n",
    "            filter_agents_threshold=cfg[\"raster_params\"][\"filter_agents_threshold\"],\n",
    "            rasterizer=rasterizer\n",
    "        )\n",
    "        \n",
    "    def get_frame(self, scene_index: int, state_index: int, track_id: Optional[int] = None) -> dict:\n",
    "        frames = self.dataset.frames[get_frames_slice_from_scenes(self.dataset.scenes[scene_index])]\n",
    "\n",
    "        tl_faces = self.dataset.tl_faces\n",
    "        try:\n",
    "            if self.cfg[\"raster_params\"][\"disable_traffic_light_faces\"]:\n",
    "                tl_faces = np.empty(0, dtype=self.dataset.tl_faces.dtype)  # completely disable traffic light faces\n",
    "        except KeyError:\n",
    "            warnings.warn(\n",
    "                \"disable_traffic_light_faces not found in config, this will raise an error in the future\",\n",
    "                RuntimeWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "        data = self.sample_function(state_index, frames, self.dataset.agents, tl_faces, track_id)\n",
    "        img = data[\"image\"]\n",
    "        k =cfg[\"model_params\"][\"history_num_frames\"]\n",
    "        imgs = [self.rasterizer.to_rgb(img[...,[i, i + k + 1, -3,-2,-1]])[np.newaxis,...] for i in range(k + 1)]\n",
    "        imgs = np.concatenate(imgs,axis = 0) / 255.\n",
    "        context_layers = np.concatenate((imgs, data[ \"deep_layers\"].transpose(2,0,1,3)), axis = -1)\n",
    "        target_positions = np.array(data[\"target_positions\"], dtype=np.float32)\n",
    "        timestamp = frames[state_index][\"timestamp\"]\n",
    "        track_id = np.int64(-1 if track_id is None else track_id)  # always a number to avoid crashing torch\n",
    "        return {\n",
    "            \"context_layers\" : context_layers,\n",
    "            \"target_positions\": target_positions,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"track_id\" : track_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAgentGenerator():\n",
    "    def __init__(self, agentdataset, number_of_samples):\n",
    "        self.agentDataset = agentdataset\n",
    "        self.number_of_samples = number_of_samples\n",
    "    \n",
    "    def __call__(self):\n",
    "        for _ in range(self.number_of_samples * 10):\n",
    "            i = np.random.randint(0,self.number_of_samples - 10)\n",
    "            yield (self.agentDataset[i][\"context_layers\"], self.agentDataset[i][\"target_positions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DecoratedAgentDataset(cfg, train_zarr, rasterizer, min_frame_history = 10, min_frame_future = 50)\n",
    "Number_of_samples = len(train_dataset)\n",
    "print(Number_of_samples)\n",
    "train_gen = TrainAgentGenerator(train_dataset, Number_of_samples)\n",
    "train_dataset = Dataset.from_generator(train_gen,(tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRNNCell(tfkl.Layer):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        self.model = model\n",
    "        self.units = model.output.shape[-1]\n",
    "        self.state_size = self.units\n",
    "        super(AgentRNNCell, self).__init__(**kwargs)\n",
    "        self.recurrent_kernel = self.add_weight( shape=(self.units, self.units),initializer='uniform',name='recurrent_kernel')\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = self.model(inputs)\n",
    "        output = h + tf.tensordot(prev_output, self.recurrent_kernel,axes = 1)\n",
    "        return output, [output]\n",
    "\n",
    "def make_cnn_model(nn):\n",
    "    for i in range(1,len(nn.layers)):\n",
    "        nn.layers[i].trainable = True\n",
    "    inputs = tfkl.Input(shape = (raster_w,raster_h,input_deep))\n",
    "    x = tfkl.Conv2D(filters = 3, kernel_size = 3, padding = \"same\")(inputs)\n",
    "    x = nn(x)\n",
    "    x = tfkl.Dropout(0.1)(x)\n",
    "    x = tfkl.Dense(1024, kernel_regularizer=tfk.regularizers.l2(0.0001))(x)\n",
    "    outputs = tfkl.LeakyReLU(0.1)(x)\n",
    "    model = tfk.Model(inputs,outputs)\n",
    "    return model\n",
    "\n",
    "def make_rnn_model(nn):\n",
    "    model_cnn = make_cnn_model(nn)\n",
    "    cell = AgentRNNCell(model_cnn)\n",
    "    inputs = tfkl.Input(shape = (length_of_rnn_sequence, raster_w,raster_h,input_deep))\n",
    "    x = tfkl.RNN(cell)(inputs)\n",
    "    x = tfkl.Dense(future_output_shape[0] * future_output_shape[1], kernel_regularizer=tfk.regularizers.l2(0.0001))(x)\n",
    "    outputs = tfkl.Reshape(future_output_shape)(x)\n",
    "    model = tfk.Model(inputs,outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predef_models = {'MobileNet' : MobileNet(include_top=False, pooling = \"avg\"),\n",
    "                'EfficientNetB0' : EfficientNetB0(include_top=False, input_shape=(224,224,3),pooling=\"avg\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_filepath = \n",
    "model = make_rnn_model(predef_models[inner_cnn_model])\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.load_weights(weight_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './weights_{loss:.2f}_{epoch:02d}.hdf5'\n",
    "#checkpoint_filepath = './best_weights.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='auto',\n",
    "    save_freq = 200,\n",
    "    save_best_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcdata = train_dataset.batch(BATCHSIZE).shuffle(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histv = model.fit(srcdata, epochs=1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights(filepath, overwrite=True, save_format=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5461134\n",
      "1778010\n",
      "6184915\n",
      "3182054\n",
      "2193135\n",
      "3666470\n",
      "5569790\n",
      "3045506\n",
      "6330240\n",
      "5656655\n",
      "3464333\n",
      "3962812\n",
      "3124512\n",
      "2049664\n",
      "4615869\n",
      "3516561\n",
      "1503886\n",
      "4991621\n",
      "5498825\n",
      "1377995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for _ in range(20):\n",
    "    print(np.random.randint(1, 7000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
