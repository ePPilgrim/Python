{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from functools import partial\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import (\n",
    "    LocalDataManager, ChunkedDataset, \n",
    "    get_agents_slice_from_frames, get_frames_slice_from_scenes,get_tl_faces_slice_from_frames,\n",
    "    filter_agents_by_labels, filter_tl_faces_by_frames\n",
    ")\n",
    "from l5kit.data.labels import PERCEPTION_LABEL_TO_INDEX \n",
    "from l5kit.data.filter import filter_agents_by_labels, filter_agents_by_track_id, filter_agents_by_frames\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import (\n",
    "    build_rasterizer, Rasterizer, RenderContext,\n",
    "    EGO_EXTENT_HEIGHT, EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH\n",
    ")\n",
    "from l5kit.rasterization.semantic_rasterizer import CV2_SHIFT, cv2_subpixel\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points, angular_distance, compute_agent_pose, rotation33_as_yaw,transform_point\n",
    "from l5kit.geometry.transform import yaw_as_rotation33\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.kinematic import Perturbation\n",
    "from l5kit.sampling.slicing import get_future_slice, get_history_slice\n",
    "from l5kit.data.zarr_dataset import AGENT_DTYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.keras.layers as tfkl\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.applications import EfficientNetB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FRAME_HISTORY = 10\n",
    "MIN_FRAME_FUTURE = 1\n",
    "DATA_DIR = \"../lyft-motion-prediction-autonomous-vehicles\"\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DATA_DIR\n",
    "dm = LocalDataManager(None)\n",
    "cfg = load_config_data(\"./pldd_agent_motion_config.yaml\")\n",
    "raster_w = cfg[\"raster_params\"][\"raster_size\"][0]\n",
    "raster_h = cfg[\"raster_params\"][\"raster_size\"][1]\n",
    "cnn_model = cfg[\"model_params\"][\"model_architecture\"]\n",
    "input_deep = 8\n",
    "length_of_rnn_sequence = cfg[\"model_params\"][\"history_num_frames\"] + 1\n",
    "future_output_shape = (cfg[\"model_params\"][\"future_num_frames\"], 2)\n",
    "EPOCHS = 128\n",
    "BATCHSIZE = cfg[\"train_data_loader\"][\"batch_size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Make training/test data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transform lift5k data source format into my model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RasterExpander():\n",
    "    def __init__(self, render_context: RenderContext, filter_agents_threshold: float, history_num_frames: int):\n",
    "        self.render_context = render_context\n",
    "        self.raster_size = render_context.raster_size_px\n",
    "        self.filter_agents_threshold = filter_agents_threshold\n",
    "        self.history_num_frames = history_num_frames\n",
    "        self.labels = ['PERCEPTION_LABEL_CAR', 'PERCEPTION_LABEL_BICYCLE', 'PERCEPTION_LABEL_PEDESTRIAN']\n",
    "        self.num_of_layers = len(self.labels) + 2    \n",
    "\n",
    "    def AddDeepness(self, history_frames: np.ndarray, history_agents: List[np.ndarray], \n",
    "                    history_tl_faces: List[np.ndarray], agent: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        frame = history_frames[0]\n",
    "        if agent is None:\n",
    "            ego_translation_m = history_frames[0][\"ego_translation\"]\n",
    "            ego_yaw_rad = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        else:\n",
    "            ego_translation_m = np.append(agent[\"centroid\"], history_frames[0][\"ego_translation\"][-1])\n",
    "            ego_yaw_rad = agent[\"yaw\"]\n",
    "\n",
    "        raster_from_world = self.render_context.raster_from_world(ego_translation_m, ego_yaw_rad)\n",
    "        out_shape = (self.raster_size[1], self.raster_size[0], self.history_num_frames + 1, self.num_of_layers)\n",
    "        agents_images = np.zeros(out_shape, dtype=np.float32)\n",
    "\n",
    "        for i, (frame, agents) in enumerate(zip(history_frames, history_agents)):\n",
    "            agents = filter_agents_by_labels(agents, self.filter_agents_threshold)\n",
    "            # note the cast is for legacy support of dataset before April 2020\n",
    "            av_agent = self.get_ego_as_agent(frame).astype(agents.dtype)\n",
    "\n",
    "            if agent is None:\n",
    "                agents_image = self.expand_into_deep(self.raster_size, raster_from_world, agents)\n",
    "            else:\n",
    "                agent_ego = filter_agents_by_track_id(agents, agent[\"track_id\"])\n",
    "                if len(agent_ego) != 0:  \n",
    "                    agents = agents[agents != agent_ego[0]]\n",
    "                agents_image = self.expand_into_deep(self.raster_size, raster_from_world, np.append(agents, av_agent))\n",
    "            agents_images[..., i,:self.num_of_layers] = agents_image.transpose((1,2,0))\n",
    "        return agents_images\n",
    "    \n",
    "    def expand_into_deep(self, raster_size: Tuple[int, int], raster_from_world: np.ndarray, agents: np.ndarray) -> np.ndarray:\n",
    "        layers = np.zeros((self.num_of_layers,raster_size[1], raster_size[0]), dtype=np.float32)\n",
    "        box_world_coords = np.zeros((len(agents), 4, 2))\n",
    "        corners_base_coords = np.asarray([[-1, -1], [-1, 1], [1, 1], [1, -1]])\n",
    "\n",
    "        for idx, agent in enumerate(agents):\n",
    "            corners = corners_base_coords * agent[\"extent\"][:2] / 2  # corners in zero\n",
    "            r_m = yaw_as_rotation33(agent[\"yaw\"])\n",
    "            box_world_coords[idx] = transform_points(corners, r_m) + agent[\"centroid\"][:2]\n",
    "    \n",
    "        box_raster_coords = transform_points(box_world_coords.reshape((-1, 2)), raster_from_world)\n",
    "        box_raster_coords = cv2_subpixel(box_raster_coords.reshape((-1, 4, 2)))\n",
    "    \n",
    "        for idx, agent in enumerate(agents):\n",
    "            #print(agent[\"label_probabilities\"])\n",
    "            for i, label in enumerate(self.labels):\n",
    "                lix = PERCEPTION_LABEL_TO_INDEX[label]\n",
    "                color = float(agent[\"label_probabilities\"][lix])\n",
    "                cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=color, shift=CV2_SHIFT)\n",
    "            i = len(self.labels)\n",
    "            cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][0]), shift=CV2_SHIFT)\n",
    "            cv2.fillPoly(layers[i + 1,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][1]),shift=CV2_SHIFT)     \n",
    "        return layers\n",
    "    \n",
    "    def get_ego_as_agent(self,frame: np.ndarray) -> np.ndarray:  # TODO this can be useful to have around\n",
    "        ego_agent = np.zeros(1, dtype=AGENT_DTYPE)\n",
    "        ego_agent[0][\"centroid\"] = frame[\"ego_translation\"][:2]\n",
    "        ego_agent[0][\"yaw\"] = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        ego_agent[0][\"extent\"] = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n",
    "        return ego_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_targets_for_deep_prediction(num_frames: int, frames: np.ndarray, selected_track_id: Optional[int], \n",
    "                                        agents: List[np.ndarray], agent_from_world: np.ndarray,current_agent_yaw: float\n",
    "                                       ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    coords_offset = np.zeros((num_frames, 2), dtype=np.float32)\n",
    "    yaws_offset = np.zeros((num_frames, 1), dtype=np.float32)\n",
    "    availability = np.zeros((num_frames,), dtype=np.float32)\n",
    "\n",
    "    for i, (frame, frame_agents) in enumerate(zip(frames, agents)):\n",
    "        if selected_track_id is None:\n",
    "            agent_centroid = frame[\"ego_translation\"][:2]\n",
    "            agent_yaw = rotation33_as_yaw(frame[\"ego_rotation\"])\n",
    "        else:\n",
    "            # it's not guaranteed the target will be in every frame\n",
    "            try:\n",
    "                agent = filter_agents_by_track_id(frame_agents, selected_track_id)[0]\n",
    "                agent_centroid = agent[\"centroid\"]\n",
    "                agent_yaw = agent[\"yaw\"]\n",
    "            except IndexError:\n",
    "                availability[i] = 0.0  # keep track of invalid futures/history\n",
    "                continue\n",
    "\n",
    "        coords_offset[i] = transform_point(agent_centroid, agent_from_world)\n",
    "        yaws_offset[i] = angular_distance(agent_yaw, current_agent_yaw)\n",
    "        availability[i] = 1.0\n",
    "    return coords_offset, yaws_offset, availability\n",
    "\n",
    "def decorated_generate_agent_sample(state_index: int, frames: np.ndarray, agents: np.ndarray, tl_faces: np.ndarray,\n",
    "                                    selected_track_id: Optional[int], render_context: RenderContext, history_num_frames: int,\n",
    "                                    history_step_size: int, future_num_frames: int, future_step_size: int,\n",
    "                                    filter_agents_threshold: float, rasterizer: Rasterizer) -> dict:\n",
    "    #  the history slice is ordered starting from the latest frame and goes backward in time., ex. slice(100, 91, -2)\n",
    "    history_slice = get_history_slice(state_index, history_num_frames, history_step_size, include_current_state=True)\n",
    "    future_slice = get_future_slice(state_index, future_num_frames, future_step_size)\n",
    "    history_frames = frames[history_slice].copy()  # copy() required if the object is a np.ndarray\n",
    "    future_frames = frames[future_slice].copy()\n",
    "    sorted_frames = np.concatenate((history_frames[::-1], future_frames))  # from past to future\n",
    "    agent_slice = get_agents_slice_from_frames(sorted_frames[0], sorted_frames[-1])\n",
    "    agents = agents[agent_slice].copy()  # this is the minimum slice of agents we need\n",
    "    history_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n",
    "    future_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n",
    "    history_agents = filter_agents_by_frames(history_frames, agents)\n",
    "    future_agents = filter_agents_by_frames(future_frames, agents)\n",
    "    tl_slice = get_tl_faces_slice_from_frames(history_frames[-1], history_frames[0])  # -1 is the farthest\n",
    "    # sync interval with the traffic light faces array\n",
    "    history_frames[\"traffic_light_faces_index_interval\"] -= tl_slice.start\n",
    "    history_tl_faces = filter_tl_faces_by_frames(history_frames, tl_faces[tl_slice].copy())\n",
    "    # State you want to predict the future of.\n",
    "    cur_frame = history_frames[0]\n",
    "    cur_agents = history_agents[0]\n",
    "\n",
    "    if selected_track_id is None:\n",
    "        agent_centroid_m = cur_frame[\"ego_translation\"][:2]\n",
    "        agent_yaw_rad = rotation33_as_yaw(cur_frame[\"ego_rotation\"])\n",
    "        agent_extent_m = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n",
    "        selected_agent = None\n",
    "    else:\n",
    "        try:\n",
    "            agent = filter_agents_by_track_id(\n",
    "                filter_agents_by_labels(cur_agents, filter_agents_threshold), selected_track_id\n",
    "            )[0]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\" track_id {selected_track_id} not in frame or below threshold\")\n",
    "        agent_centroid_m = agent[\"centroid\"]\n",
    "        agent_yaw_rad = float(agent[\"yaw\"])\n",
    "        agent_extent_m = agent[\"extent\"]\n",
    "        selected_agent = agent\n",
    "\n",
    "    input_im = (\n",
    "        None\n",
    "        if not rasterizer\n",
    "        else rasterizer.rasterize(history_frames, history_agents, history_tl_faces, selected_agent)\n",
    "    )\n",
    "    \n",
    "    rasterExpander = RasterExpander(rasterizer.render_context, rasterizer.filter_agents_threshold,rasterizer.history_num_frames)\n",
    "    deepLayers = rasterExpander.AddDeepness(history_frames, history_agents, history_tl_faces, selected_agent)\n",
    "\n",
    "    world_from_agent = compute_agent_pose(agent_centroid_m, agent_yaw_rad)\n",
    "    agent_from_world = np.linalg.inv(world_from_agent)\n",
    "    raster_from_world = render_context.raster_from_world(agent_centroid_m, agent_yaw_rad)\n",
    "\n",
    "    future_coords_offset, future_yaws_offset, future_availability = _create_targets_for_deep_prediction(\n",
    "        future_num_frames, future_frames, selected_track_id, future_agents, agent_from_world, agent_yaw_rad\n",
    "    )\n",
    "\n",
    "    # history_num_frames + 1 because it also includes the current frame\n",
    "    history_coords_offset, history_yaws_offset, history_availability = _create_targets_for_deep_prediction(\n",
    "        history_num_frames + 1, history_frames, selected_track_id, history_agents, agent_from_world, agent_yaw_rad\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"image\": input_im,\n",
    "        \"deep_layers\": deepLayers,\n",
    "        \"target_positions\": future_coords_offset,\n",
    "        \"target_yaws\": future_yaws_offset,\n",
    "        \"target_availabilities\": future_availability,\n",
    "        \"history_positions\": history_coords_offset,\n",
    "        \"history_yaws\": history_yaws_offset,\n",
    "        \"history_availabilities\": history_availability,\n",
    "        \"world_to_image\": raster_from_world,  # TODO deprecate\n",
    "        \"raster_from_agent\": raster_from_world @ world_from_agent,\n",
    "        \"raster_from_world\": raster_from_world,\n",
    "        \"agent_from_world\": agent_from_world,\n",
    "        \"world_from_agent\": world_from_agent,\n",
    "        \"centroid\": agent_centroid_m,\n",
    "        \"yaw\": agent_yaw_rad,\n",
    "        \"extent\": agent_extent_m,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoratedAgentDataset(AgentDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: dict,\n",
    "        zarr_dataset: ChunkedDataset,\n",
    "        rasterizer: Rasterizer,\n",
    "        perturbation: Optional[Perturbation] = None,\n",
    "        agents_mask: Optional[np.ndarray] = None,\n",
    "        min_frame_history: int = MIN_FRAME_HISTORY,\n",
    "        min_frame_future: int = MIN_FRAME_FUTURE,\n",
    "    ):\n",
    "        assert perturbation is None, \"AgentDataset does not support perturbation (yet)\"\n",
    "        super(DecoratedAgentDataset, self).__init__(cfg, zarr_dataset, rasterizer, perturbation, agents_mask,\n",
    "                                                    min_frame_history, min_frame_future)\n",
    "        render_context = RenderContext(\n",
    "            raster_size_px=np.array(cfg[\"raster_params\"][\"raster_size\"]),\n",
    "            pixel_size_m=np.array(cfg[\"raster_params\"][\"pixel_size\"]),\n",
    "            center_in_raster_ratio=np.array(cfg[\"raster_params\"][\"ego_center\"]),\n",
    "        )\n",
    "        self.sample_function = partial(\n",
    "            decorated_generate_agent_sample,\n",
    "            render_context=render_context,\n",
    "            history_num_frames=cfg[\"model_params\"][\"history_num_frames\"],\n",
    "            history_step_size=cfg[\"model_params\"][\"history_step_size\"],\n",
    "            future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n",
    "            future_step_size=cfg[\"model_params\"][\"future_step_size\"],\n",
    "            filter_agents_threshold=cfg[\"raster_params\"][\"filter_agents_threshold\"],\n",
    "            rasterizer=rasterizer\n",
    "        )\n",
    "        \n",
    "    def get_frame(self, scene_index: int, state_index: int, track_id: Optional[int] = None) -> dict:\n",
    "        frames = self.dataset.frames[get_frames_slice_from_scenes(self.dataset.scenes[scene_index])]\n",
    "\n",
    "        tl_faces = self.dataset.tl_faces\n",
    "        try:\n",
    "            if self.cfg[\"raster_params\"][\"disable_traffic_light_faces\"]:\n",
    "                tl_faces = np.empty(0, dtype=self.dataset.tl_faces.dtype)  # completely disable traffic light faces\n",
    "        except KeyError:\n",
    "            warnings.warn(\n",
    "                \"disable_traffic_light_faces not found in config, this will raise an error in the future\",\n",
    "                RuntimeWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "        data = self.sample_function(state_index, frames, self.dataset.agents, tl_faces, track_id)\n",
    "        img = data[\"image\"]\n",
    "        k =cfg[\"model_params\"][\"history_num_frames\"]\n",
    "        imgs = [self.rasterizer.to_rgb(img[...,[i, i + k + 1, -3,-2,-1]])[np.newaxis,...] for i in range(k + 1)]\n",
    "        imgs = np.concatenate(imgs,axis = 0) / 255.\n",
    "        context_layers = np.concatenate((imgs, data[ \"deep_layers\"].transpose(2,0,1,3)), axis = -1)\n",
    "        target_positions = np.array(data[\"target_positions\"], dtype=np.float32)\n",
    "        timestamp = frames[state_index][\"timestamp\"]\n",
    "        track_id = np.int64(-1 if track_id is None else track_id)  # always a number to avoid crashing torch\n",
    "        return {\n",
    "            \"context_layers\" : context_layers,\n",
    "            \"target_positions\": target_positions,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"track_id\" : track_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 To the tensorflow dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAgentGenerator():\n",
    "    def __init__(self, agentdataset):\n",
    "        self.agentDataset = agentdataset\n",
    "    \n",
    "    def __call__(self):\n",
    "        for data in self.agentDataset:\n",
    "            yield (data[\"context_layers\"], data[\"target_positions\"])\n",
    "            \n",
    "class TestAgentGenerator():\n",
    "    def __init__(self, agentdataset):\n",
    "        self.agentDataset = agentdataset\n",
    "    \n",
    "    def __call__(self):\n",
    "        for data in self.agentDataset:\n",
    "            yield (data[\"context_layers\"], data[\"target_positions\"],data[\"timestamp\"], data[\"track_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Get train dataset source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\demyd\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: you're running with custom min_frame_future of 50\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7335740\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DecoratedAgentDataset(cfg, train_zarr, rasterizer, min_frame_history = 10, min_frame_future = 50)\n",
    "print(len(train_dataset))\n",
    "train_gen = TrainAgentGenerator(train_dataset)\n",
    "train_dataset = Dataset.from_generator(train_gen,(tf.float32, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Get test dataset source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg = cfg[\"test_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
    "test_mask = np.load(f\"{DATA_DIR}/scenes/mask.npz\")[\"arr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DecoratedAgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n",
    "test_gen = TestAgentGenerator(test_dataset)\n",
    "test_dataset = Dataset.from_generator(test_gen,(tf.float32, tf.float32, tf.int64, tf.uint64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Design ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRNNCell(tfkl.Layer):\n",
    "    def __init__(self, model, **kwargs):\n",
    "        self.model = model\n",
    "        self.units = model.output.shape[-1]\n",
    "        self.state_size = self.units\n",
    "        super(AgentRNNCell, self).__init__(**kwargs)\n",
    "        self.recurrent_kernel = self.add_weight( shape=(self.units, self.units),initializer='uniform',name='recurrent_kernel')\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = self.model(inputs)\n",
    "        output = h + tf.tensordot(prev_output, self.recurrent_kernel,axes = 1)\n",
    "        return output, [output]\n",
    "\n",
    "def make_cnn_model(nn):\n",
    "    for i in range(1,len(nn.layers)):\n",
    "        nn.layers[i].trainable = True\n",
    "    inputs = tfkl.Input(shape = (raster_w,raster_h,input_deep))\n",
    "    x = tfkl.Conv2D(filters = 3, kernel_size = 3, padding = \"same\")(inputs)\n",
    "    x = nn(x)\n",
    "    x = tfkl.Dropout(0.1)(x)\n",
    "    x = tfkl.Dense(1024, kernel_regularizer=tfk.regularizers.l2(0.0001))(x)\n",
    "    outputs = tfkl.LeakyReLU(0.1)(x)\n",
    "    model = tfk.Model(inputs,outputs)\n",
    "    return model\n",
    "\n",
    "def make_rnn_model(nn):\n",
    "    model_cnn = make_cnn_model(nn)\n",
    "    cell = AgentRNNCell(model_cnn)\n",
    "    inputs = tfkl.Input(shape = (length_of_rnn_sequence, raster_w,raster_h,input_deep))\n",
    "    x = tfkl.RNN(cell)(inputs)\n",
    "    x = tfkl.Dense(future_output_shape[0] * future_output_shape[1], kernel_regularizer=tfk.regularizers.l2(0.0001))(x)\n",
    "    outputs = tfkl.Reshape(future_output_shape)(x)\n",
    "    model = tfk.Model(inputs,outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "predef_models = {'MobileNet' : MobileNet(include_top=False, pooling = \"avg\"),\n",
    "                'EfficientNetB0' : EfficientNetB0(include_top=False, input_shape=(224,224,3),pooling=\"avg\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_rnn_model(predef_models[cnn_model])\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '.\\\\checkpoint\\\\weights_{loss:.2f}_{epoch:02d}.hdf5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='auto',\n",
    "    save_best_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcdata = train_dataset.batch(BATCHSIZE).shuffle(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "    131/Unknown - 8151s 62s/step - loss: 8.8264 - mae: 0.9512"
     ]
    }
   ],
   "source": [
    "histv = model.fit(srcdata, epochs=EPOCHS, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2 = make_rnn_model(predef_models[cnn_model])\n",
    "#model2.load_weights(\".\\\\checkpoint\\\\weights_0.31_03.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCHSIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(test_dataset))[0]\n",
    "plt.imshow(img[0,...].numpy()[...,[0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(train_dataset))[0]\n",
    "plt.imshow(img[0,...].numpy()[...,[0,1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a3 = np.array( [[[0,0],[100,10],[100,100],[10,100]]], dtype=np.int32 )\n",
    "im = np.zeros([11,240,320],dtype=np.float32)\n",
    "cv2.fillPoly( im[0,...], a3, -0.001 )\n",
    "\n",
    "plt.imshow(im[0,...])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.transpose((2,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
