{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport warnings\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom functools import partial\nfrom tempfile import gettempdir\nimport matplotlib.pyplot as plt\nimport time\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nfrom pathlib import Path\nimport os\nimport cv2\n\n#-----------------l5kit lib ------------------------------------------\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import (\n    LocalDataManager, ChunkedDataset, DataManager,\n    get_agents_slice_from_frames, get_frames_slice_from_scenes,get_tl_faces_slice_from_frames,\n    filter_agents_by_labels, filter_tl_faces_by_frames\n)\nfrom l5kit.data.labels import PERCEPTION_LABEL_TO_INDEX \nfrom l5kit.data.filter import filter_agents_by_labels, filter_agents_by_track_id, filter_agents_by_frames\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import (\n    build_rasterizer, Rasterizer, RenderContext,\n    EGO_EXTENT_HEIGHT, EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH\n)\nfrom l5kit.rasterization.sem_box_rasterizer import SemBoxRasterizer\nfrom l5kit.rasterization.semantic_rasterizer import CV2_SHIFT, cv2_subpixel,SemanticRasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points, angular_distance, compute_agent_pose, rotation33_as_yaw,transform_point\nfrom l5kit.geometry.transform import yaw_as_rotation33\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\nfrom l5kit.kinematic import Perturbation\nfrom l5kit.sampling.slicing import get_future_slice, get_history_slice\nfrom l5kit.data.zarr_dataset import AGENT_DTYPE\n\n#------------ Tensorflow lib ---------------------------------------------\nimport tensorflow as tf\nimport tensorflow.keras as tfk\nimport tensorflow.keras.layers as tfkl\nfrom tensorflow.data import Dataset\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n#\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cfg = {\n    'format_version': 4,\n    'model_params': {\n        'history_num_frames': 10,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map/aerial_map.png',\n        'semantic_map_key': 'semantic_map/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5,\n        'disable_traffic_light_faces': False\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes/test.zarr',\n        'weights' : \"/kaggle/input/lift5k-weights-mobilenet/weight_mobile_net_19229.hdf5\",\n        'load_weights' : False,\n        'input_deep' : 69,\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 4\n    }\n}\n\nDIR_INPUT = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\nMIN_FRAME_HISTORY = 10\nMIN_FRAME_FUTURE = 1\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\ndm = LocalDataManager(None)\nraster_w = cfg[\"raster_params\"][\"raster_size\"][0]\nraster_h = cfg[\"raster_params\"][\"raster_size\"][1]\ninput_deep = cfg[\"train_data_loader\"][\"input_deep\"]\nlength_of_rnn_sequence = cfg[\"model_params\"][\"history_num_frames\"] + 1\nfuture_output_shape = (cfg[\"model_params\"][\"future_num_frames\"], 2)\nBATCHSIZE = cfg[\"train_data_loader\"][\"batch_size\"]\ntrain_zarr = ChunkedDataset(dm.require(cfg[\"train_data_loader\"][\"key\"])).open()\nweights_filepath = cfg[\"train_data_loader\"][\"weights\"]\nload_weights = cfg[\"train_data_loader\"][\"load_weights\"]\nmob_net = tfk.applications.MobileNet(include_top=False, pooling = \"avg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_into_deep(num_of_layers, raster_size: Tuple[int, int], raster_from_world: np.ndarray, agents: np.ndarray,\n                     ego_index: int) -> np.ndarray:\n    layers = np.zeros((num_of_layers,raster_size[1], raster_size[0]), dtype=np.float32)\n    box_world_coords = np.zeros((len(agents), 4, 2))\n    corners_base_coords = np.asarray([[-1, -1], [-1, 1], [1, 1], [1, -1]])\n\n    for idx, agent in enumerate(agents):\n        corners = corners_base_coords * agent[\"extent\"][:2] / 2  # corners in zero\n        r_m = yaw_as_rotation33(agent[\"yaw\"])\n        box_world_coords[idx] = transform_points(corners, r_m) + agent[\"centroid\"][:2]\n    \n    box_raster_coords = transform_points(box_world_coords.reshape((-1, 2)), raster_from_world)\n    box_raster_coords = cv2_subpixel(box_raster_coords.reshape((-1, 4, 2)))\n    labels = ['PERCEPTION_LABEL_CAR', 'PERCEPTION_LABEL_BICYCLE', 'PERCEPTION_LABEL_PEDESTRIAN']\n        \n    for idx, agent in enumerate(agents):\n        for i, label in enumerate(labels):\n            lix = PERCEPTION_LABEL_TO_INDEX[label]\n            color = 1.0 - float(agent[\"label_probabilities\"][lix])\n            cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=color, shift=CV2_SHIFT)\n        i = len(labels)\n        cv2.fillPoly(layers[i,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][0]), shift=CV2_SHIFT)\n        cv2.fillPoly(layers[i + 1,...], box_raster_coords[[idx]], color=float(agent[\"velocity\"][1]),shift=CV2_SHIFT) \n                \n    if ego_index >= 0:\n        cv2.fillPoly(layers[num_of_layers - 1,...], box_raster_coords[[ego_index]], color=1.0, shift=CV2_SHIFT) \n    return layers\n\ndef get_ego_as_agent(frame: np.ndarray) -> np.ndarray:  # TODO this can be useful to have around\n    ego_agent = np.zeros(1, dtype=AGENT_DTYPE)\n    ego_agent[0][\"centroid\"] = frame[\"ego_translation\"][:2]\n    ego_agent[0][\"yaw\"] = rotation33_as_yaw(frame[\"ego_rotation\"])\n    ego_agent[0][\"extent\"] = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n    return ego_agent\n\ndef _load_metadata(meta_key: str, data_manager: DataManager) -> dict:\n    metadata_path = data_manager.require(meta_key)\n    with open(metadata_path, \"r\") as f:\n        metadata: dict = json.load(f)\n    return metadata\n\nclass DecoratedSemBoxRasterizer(Rasterizer):\n    def __init__(self,cfg: dict, data_manager: DataManager):\n        super(DecoratedSemBoxRasterizer, self).__init__()\n        raster_cfg = cfg[\"raster_params\"]\n        map_type = raster_cfg[\"map_type\"]\n        dataset_meta_key = raster_cfg[\"dataset_meta_key\"]\n\n        render_context = RenderContext(\n            raster_size_px=np.array(raster_cfg[\"raster_size\"]),\n            pixel_size_m=np.array(raster_cfg[\"pixel_size\"]),\n            center_in_raster_ratio=np.array(raster_cfg[\"ego_center\"]),)\n\n        filter_agents_threshold = raster_cfg[\"filter_agents_threshold\"]\n        history_num_frames = cfg[\"model_params\"][\"history_num_frames\"]\n        semantic_map_path = data_manager.require(raster_cfg[\"semantic_map_key\"])\n        dataset_meta = _load_metadata(raster_cfg[\"dataset_meta_key\"], data_manager)\n        world_to_ecef = np.array(dataset_meta[\"world_to_ecef\"], dtype=np.float64)\n              \n        self.render_context = render_context\n        self.raster_size = render_context.raster_size_px\n        self.pixel_size = render_context.pixel_size_m\n        self.ego_center = render_context.center_in_raster_ratio\n        self.filter_agents_threshold = filter_agents_threshold\n        self.history_num_frames = history_num_frames\n        self.sem_rast = SemanticRasterizer(render_context, semantic_map_path, world_to_ecef)\n\n    def rasterize(self,history_frames: np.ndarray,history_agents: List[np.ndarray],\n                  history_tl_faces: List[np.ndarray],agent: Optional[np.ndarray] = None,) -> np.ndarray:\n        im_out_box = self.get_box_rasterization(history_frames,\n                                                history_agents,\n                                                history_tl_faces,\n                                                agent ).reshape(self.raster_size[0], self.raster_size[1], -1)\n        im_out_sem = self.sem_rast.rasterize(history_frames, history_agents, history_tl_faces, agent)\n        return np.concatenate([im_out_box, im_out_sem], -1)\n    \n    def get_box_rasterization(self, history_frames: np.ndarray, history_agents: List[np.ndarray],history_tl_faces: List[np.ndarray],\n                              agent: Optional[np.ndarray] = None):\n        if agent is None:\n            ego_translation_m = history_frames[0][\"ego_translation\"]\n            ego_yaw_rad = rotation33_as_yaw(frame[\"ego_rotation\"])\n        else:\n            ego_translation_m = np.append(agent[\"centroid\"], history_frames[0][\"ego_translation\"][-1])\n            ego_yaw_rad = agent[\"yaw\"]\n        num_of_layers = 6\n        raster_from_world = self.render_context.raster_from_world(ego_translation_m, ego_yaw_rad)\n        out_shape = (self.raster_size[1], self.raster_size[0], self.history_num_frames + 1, num_of_layers)\n        agents_images = np.zeros(out_shape, dtype=np.float32)\n        \n        for i, (frame, agents) in enumerate(zip(history_frames, history_agents)):\n            agents = filter_agents_by_labels(agents, self.filter_agents_threshold)\n            av_index = len(agents)\n            # note the cast is for legacy support of dataset before April 2020\n            av_agent = get_ego_as_agent(frame).astype(agents.dtype)\n            if agent is None:\n                agents_image = expand_into_deep(num_of_layers, self.raster_size, raster_from_world,\n                                                np.append(agents, av_agent), ego_index = -1)\n            else:\n                agent_ego = filter_agents_by_track_id(agents, agent[\"track_id\"])\n                if len(agent_ego) == 0:  # agent not in this history frame\n                    agents_image = expand_into_deep(num_of_layers,self.raster_size, raster_from_world, np.append(agents, av_agent),\n                                                    ego_index = -1)\n                else:  # add av to agents and remove the agent from agents\n                    #agents = agents[agents != agent_ego[0]]\n                    ego_index = np.nonzero(agents == agent_ego[0])[0][0]\n                    agents_image = expand_into_deep(num_of_layers,self.raster_size, raster_from_world, np.append(agents, av_agent), ego_index)\n            agents_images[..., i,:num_of_layers] = agents_image.transpose((1,2,0))\n        return agents_images\n    \n    def to_rgb(self, in_im: np.ndarray, **kwargs: dict) -> np.ndarray:\n        pass\n    \ndef _create_targets_for_deep_prediction(num_frames: int, frames: np.ndarray, selected_track_id: Optional[int], \n                                        agents: List[np.ndarray], agent_from_world: np.ndarray,current_agent_yaw: float\n                                       ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    coords_offset = np.zeros((num_frames, 2), dtype=np.float32)\n    yaws_offset = np.zeros((num_frames, 1), dtype=np.float32)\n    velocity_offset = np.zeros((num_frames, 2), dtype=np.float32)\n    availability = np.zeros((num_frames,), dtype=np.float32)\n\n    for i, (frame, frame_agents) in enumerate(zip(frames, agents)):\n        if selected_track_id is None:\n            agent_centroid = frame[\"ego_translation\"][:2]\n            agent_velocity = np.zeros(2,dtype = np.float32)\n            agent_yaw = rotation33_as_yaw(frame[\"ego_rotation\"])\n        else:\n            # it's not guaranteed the target will be in every frame\n            try:\n                agent = filter_agents_by_track_id(frame_agents, selected_track_id)[0]\n                agent_centroid = agent[\"centroid\"]\n                agent_velocity = agent[\"velocity\"]\n                agent_yaw = agent[\"yaw\"]\n            except IndexError:\n                availability[i] = 0.0  # keep track of invalid futures/history\n                continue\n\n        coords_offset[i] = transform_point(agent_centroid, agent_from_world)\n        yaws_offset[i] = angular_distance(agent_yaw, current_agent_yaw)\n        velocity_offset[i] = agent_velocity\n        availability[i] = 1.0\n    return coords_offset, yaws_offset, velocity_offset \n\ndef decorated_generate_agent_sample(state_index: int, frames: np.ndarray, agents: np.ndarray, tl_faces: np.ndarray,\n                                    selected_track_id: Optional[int], render_context: RenderContext, history_num_frames: int,\n                                    history_step_size: int, future_num_frames: int, future_step_size: int,\n                                    filter_agents_threshold: float, rasterizer: Rasterizer) -> dict:\n    #  the history slice is ordered starting from the latest frame and goes backward in time., ex. slice(100, 91, -2)\n    history_slice = get_history_slice(state_index, history_num_frames, history_step_size, include_current_state=True)\n    future_slice = get_future_slice(state_index, future_num_frames, future_step_size)\n    history_frames = frames[history_slice].copy()  # copy() required if the object is a np.ndarray\n    future_frames = frames[future_slice].copy()\n    sorted_frames = np.concatenate((history_frames[::-1], future_frames))  # from past to future\n    agent_slice = get_agents_slice_from_frames(sorted_frames[0], sorted_frames[-1])\n    agents = agents[agent_slice].copy()  # this is the minimum slice of agents we need\n    history_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n    future_frames[\"agent_index_interval\"] -= agent_slice.start  # sync interval with the agents array\n    history_agents = filter_agents_by_frames(history_frames, agents)\n    future_agents = filter_agents_by_frames(future_frames, agents)\n    tl_slice = get_tl_faces_slice_from_frames(history_frames[-1], history_frames[0])  # -1 is the farthest\n    # sync interval with the traffic light faces array\n    history_frames[\"traffic_light_faces_index_interval\"] -= tl_slice.start\n    history_tl_faces = filter_tl_faces_by_frames(history_frames, tl_faces[tl_slice].copy())\n    # State you want to predict the future of.\n    cur_frame = history_frames[0]\n    cur_agents = history_agents[0]\n\n    if selected_track_id is None:\n        agent_centroid_m = cur_frame[\"ego_translation\"][:2]\n        agent_yaw_rad = rotation33_as_yaw(cur_frame[\"ego_rotation\"])\n        agent_extent_m = np.asarray((EGO_EXTENT_LENGTH, EGO_EXTENT_WIDTH, EGO_EXTENT_HEIGHT))\n        agent_velocity_m = np.zero(2, dtype=np.float32)\n        selected_agent = None\n    else:\n        try:\n            agent = filter_agents_by_track_id(\n                filter_agents_by_labels(cur_agents, filter_agents_threshold), selected_track_id\n            )[0]\n        except IndexError:\n            raise ValueError(f\" track_id {selected_track_id} not in frame or below threshold\")\n        agent_centroid_m = agent[\"centroid\"]\n        agent_yaw_rad = float(agent[\"yaw\"])\n        agent_extent_m = agent[\"extent\"]\n        agent_velocity_m = agent[\"velocity\"]\n        selected_agent = agent\n\n    input_im = (\n        None\n        if not rasterizer\n        else rasterizer.rasterize(history_frames, history_agents, history_tl_faces, selected_agent)\n    )\n\n    world_from_agent = compute_agent_pose(agent_centroid_m, agent_yaw_rad)\n    agent_from_world = np.linalg.inv(world_from_agent)\n    raster_from_world = render_context.raster_from_world(agent_centroid_m, agent_yaw_rad)\n\n    future_coords_offset, future_yaws_offset, future_velocity_offset = _create_targets_for_deep_prediction(\n        future_num_frames, future_frames, selected_track_id, future_agents, agent_from_world, agent_yaw_rad\n    )\n\n    return {\n        \"image\": input_im,\n        \"target_positions\": future_coords_offset,\n        \"target_velocity\" : future_velocity_offset,\n        \"target_yaws\": future_yaws_offset,\n        \"centroid\": agent_centroid_m,\n        \"yaw\": agent_yaw_rad,\n        \"extent\": agent_extent_m,\n        \"velocity\": agent_velocity_m\n    }\n\nclass DecoratedAgentDataset(AgentDataset):\n    def __init__(\n        self,\n        cfg: dict,\n        zarr_dataset: ChunkedDataset,\n        rasterizer: Rasterizer,\n        perturbation: Optional[Perturbation] = None,\n        agents_mask: Optional[np.ndarray] = None,\n        min_frame_history: int = MIN_FRAME_HISTORY,\n        min_frame_future: int = MIN_FRAME_FUTURE,\n    ):\n        assert perturbation is None, \"AgentDataset does not support perturbation (yet)\"\n        super(DecoratedAgentDataset, self).__init__(cfg, zarr_dataset, rasterizer, perturbation, agents_mask,\n                                                    min_frame_history, min_frame_future)\n        render_context = RenderContext(\n            raster_size_px=np.array(cfg[\"raster_params\"][\"raster_size\"]),\n            pixel_size_m=np.array(cfg[\"raster_params\"][\"pixel_size\"]),\n            center_in_raster_ratio=np.array(cfg[\"raster_params\"][\"ego_center\"]),\n        )\n        self.sample_function = partial(\n            decorated_generate_agent_sample,\n            render_context=render_context,\n            history_num_frames=cfg[\"model_params\"][\"history_num_frames\"],\n            history_step_size=cfg[\"model_params\"][\"history_step_size\"],\n            future_num_frames=cfg[\"model_params\"][\"future_num_frames\"],\n            future_step_size=cfg[\"model_params\"][\"future_step_size\"],\n            filter_agents_threshold=cfg[\"raster_params\"][\"filter_agents_threshold\"],\n            rasterizer=rasterizer\n        )\n        \n    def get_frame(self, scene_index: int, state_index: int, track_id: Optional[int] = None) -> dict:\n        frames = self.dataset.frames[get_frames_slice_from_scenes(self.dataset.scenes[scene_index])]\n\n        tl_faces = self.dataset.tl_faces\n        try:\n            if self.cfg[\"raster_params\"][\"disable_traffic_light_faces\"]:\n                tl_faces = np.empty(0, dtype=self.dataset.tl_faces.dtype)  # completely disable traffic light faces\n        except KeyError:\n            warnings.warn(\n                \"disable_traffic_light_faces not found in config, this will raise an error in the future\",\n                RuntimeWarning,\n                stacklevel=2,\n            )\n        data = self.sample_function(state_index, frames, self.dataset.agents, tl_faces, track_id)\n        imgs = data[\"image\"]\n        k =cfg[\"model_params\"][\"history_num_frames\"]\n        target_positions = np.array(data[\"target_positions\"], dtype=np.float32)\n        target_velocity = np.array(data[\"target_velocity\"], dtype=np.float32)\n        target_yaws = np.array(data[\"target_yaws\"], dtype=np.float32)\n        return {\n            \"deep_layers\" : imgs,\n            \"target_positions\": target_positions,\n            \"target_velocity\": target_velocity,\n            \"target_yaws\" : target_yaws\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_input_model():\n    inputs = tfkl.Input(shape = (224,224,69))\n    x = list()\n    i = 0\n    for j in range(6,67,6):\n        t = tfkl.Conv2D(3, 7, padding=\"same\")(tfkl.concatenate([inputs[...,i:j],inputs[...,-3:]]))\n        t = tfkl.LeakyReLU(0.001)(t) \n        x.append(t) \n        i = j \n    t = tfkl.Conv2D(24, 5, padding=\"same\")(tfkl.concatenate(x))\n    t = tfkl.LeakyReLU(0.001)(t)\n    t = tfkl.Conv2D(12, 3, padding=\"same\")(t)\n    t = tfkl.LeakyReLU(0.001)(t)\n    t = tfkl.Conv2D(6, 3, padding=\"same\")(t)\n    t = tfkl.LeakyReLU(0.001)(t)\n    t = tfkl.Conv2D(3, 3, padding=\"same\")(t)\n    curr_x = tfkl.LeakyReLU(0.001)(t)\n    '''\n    curr_x = x[0]\n    for idx in range(1,len(x)):\n        t = tfkl.Conv2D(3, 5, padding=\"same\")(tfkl.concatenate([curr_x,x[idx]]))\n        t = tfkl.LeakyReLU(0.001)(t)\n        #t = tfkl.Conv2D(4, 3, padding=\"same\")(t)\n        #t = tfkl.LeakyReLU(0.001)(t) \n        curr_x = t #tfkl.Conv2D(3, 3, activation='relu', padding=\"same\")(t) \n    '''\n    model = tfk.Model(inputs,curr_x)\n    return model\n\ndef make_cnn_model(nn):\n    for i in range(1,len(nn.layers)):\n        nn.layers[i].trainable = True\n    inputs = tfkl.Input(shape = (raster_w,raster_h,input_deep))\n    x = make_input_model()(inputs)\n    x = nn(x)\n    x = tfkl.Dropout(0.1)(x)\n    x = tfkl.Dense(future_output_shape[0] * future_output_shape[1], kernel_regularizer=tfk.regularizers.l2(0.0001))(x)\n    outputs = tfkl.Reshape(future_output_shape)(x)\n    model = tfk.Model(inputs,outputs)\n    return model\n\n\nmodel = make_cnn_model(mob_net)\nmodel.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\nif load_weights:\n    model.load_weights(weights_filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cfg = cfg[\"train_data_loader\"]\nrasterizer = DecoratedSemBoxRasterizer(cfg, dm)\ntrain_dataset = DecoratedAgentDataset(cfg, train_zarr, rasterizer, min_frame_history = 10, min_frame_future = 50)\nNumber_of_samples = len(train_dataset)\nprint(Number_of_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainAgentGenerator():\n    def __init__(self, agentdataset, number_of_samples):\n        self.agentDataset = agentdataset\n        self.number_of_samples = number_of_samples\n    \n    def __call__(self):\n        for _ in range(self.number_of_samples * 10):\n            i = np.random.randint(0,self.number_of_samples - 10)\n            yield (self.agentDataset[i][\"deep_layers\"], self.agentDataset[i][\"target_positions\"])\n            \ntrain_gen = TrainAgentGenerator(train_dataset, Number_of_samples)\nsrc_dataset = Dataset.from_generator(train_gen,(tf.float32, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"srcdata = src_dataset.batch(BATCHSIZE).prefetch(tf.data.experimental.AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"histv = model.fit(srcdata, epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.save_weights('./one_epoch_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for d,t in srcdata:\n#    y = model.evaluate(d,t,verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}